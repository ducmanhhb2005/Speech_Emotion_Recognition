{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1775506,
     "status": "ok",
     "timestamp": 1766749239861,
     "user": {
      "displayName": "Duc Manh Deanjames",
      "userId": "07257301992281346172"
     },
     "user_tz": -420
    },
    "id": "Qpi6TuquVTTV",
    "outputId": "0f805821-7f45-410b-b8fc-96960b8a2ade"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from google.colab import drive\n",
    "\n",
    "\n",
    "print(\"Connect Drive\")\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "BASE_DIR = \"/content/drive/MyDrive/Speech_Emotion_Recognition\"\n",
    "INPUT_DIR = os.path.join(BASE_DIR, \"input\")\n",
    "RAVDESS_DIR = os.path.join(INPUT_DIR, \"ravdess-emotional-speech-audio\")\n",
    "TESS_DIR = os.path.join(INPUT_DIR, \"TESS Toronto emotional speech set data\")\n",
    "\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"working/processed\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "#Emotion mapping\n",
    "RAVDESS_MAP = {\n",
    "    '01': 'neutral', '02': 'neutral', \n",
    "    '03': 'happy',   '04': 'sad',\n",
    "    '05': 'angry',   '06': 'fearful',\n",
    "    '07': 'disgust', '08': 'surprised'\n",
    "}\n",
    "\n",
    "TESS_MAP = {\n",
    "    'neutral': 'neutral',\n",
    "    'happy': 'happy',\n",
    "    'sad': 'sad',\n",
    "    'angry': 'angry',\n",
    "    'fear': 'fearful',\n",
    "    'disgust': 'disgust',\n",
    "    'ps': 'surprised'\n",
    "}\n",
    "\n",
    "EMOTION_TO_ID = {\n",
    "    'neutral': 0, 'happy': 1, 'sad': 2, 'angry': 3,\n",
    "    'fearful': 4, 'disgust': 5, 'surprised': 6\n",
    "}\n",
    "\n",
    "\n",
    "def preprocess_audio(file_path, target_sr=16000):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "        if y.ndim > 1: y = librosa.to_mono(y)\n",
    "        y_resampled = librosa.resample(y, orig_sr=sr, target_sr=target_sr)\n",
    "        return y_resampled, target_sr\n",
    "    except Exception as e:\n",
    "        return None, None\n",
    "\n",
    "all_data = []\n",
    "\n",
    "\n",
    "# RAVDESS\n",
    "for root, _, files in os.walk(RAVDESS_DIR):\n",
    "    for f in files:\n",
    "        if f.endswith(\".wav\"):\n",
    "            parts = f.split('-')\n",
    "            if len(parts) >= 3:\n",
    "                emotion_name = RAVDESS_MAP.get(parts[2])\n",
    "                if emotion_name:\n",
    "                    all_data.append({\n",
    "                        'raw_path': os.path.join(root, f),\n",
    "                        'label_id': EMOTION_TO_ID[emotion_name]\n",
    "                    })\n",
    "\n",
    "# TESS\n",
    "for root, _, files in os.walk(TESS_DIR):\n",
    "    for f in files:\n",
    "        if f.endswith(\".wav\"):\n",
    "            emo = f.lower().split('.')[0].split('_')[-1]\n",
    "            emotion_name = TESS_MAP.get(emo)\n",
    "            if emotion_name:\n",
    "                all_data.append({\n",
    "                    'raw_path': os.path.join(root, f),\n",
    "                    'label_id': EMOTION_TO_ID[emotion_name]\n",
    "                })\n",
    "\n",
    "df_raw = pd.DataFrame(all_data)\n",
    "print(f\"Total: {len(df_raw)} \")\n",
    "print(\"Data distribution\\n\", df_raw['label_id'].value_counts())\n",
    "\n",
    "#Split train/test set\n",
    "df_train_full, df_test = train_test_split(\n",
    "    df_raw, test_size=0.2, stratify=df_raw['label_id'], random_state=42\n",
    ")\n",
    "\n",
    "#Split valid\n",
    "df_train, df_val = train_test_split(\n",
    "    df_train_full, test_size=0.15, stratify=df_train_full['label_id'], random_state=42\n",
    ")\n",
    "\n",
    "#Oversampling\n",
    "print(\"\\nBlancing Train Set...\")\n",
    "max_size = df_train['label_id'].value_counts().max()\n",
    "lst_balanced = []\n",
    "\n",
    "for class_id in df_train['label_id'].unique():\n",
    "    df_class = df_train[df_train['label_id'] == class_id]\n",
    "    df_class_oversampled = resample(df_class,\n",
    "                                    replace=True,     \n",
    "                                    n_samples=max_size, \n",
    "                                    random_state=42)\n",
    "    lst_balanced.append(df_class_oversampled)\n",
    "\n",
    "df_train_balanced = pd.concat(lst_balanced)\n",
    "print(\"Data distribution after balancing\\n\", df_train_balanced['label_id'].value_counts())\n",
    "\n",
    "#preprocessing \n",
    "def process_and_save_dataset(df, folder_name):\n",
    "    print(f\"Working on {folder_name}...\")\n",
    "    unique_files = df['raw_path'].unique()\n",
    "    path_map = {}\n",
    "\n",
    "    save_path = os.path.join(OUTPUT_DIR, folder_name)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    for i, raw_f in enumerate(unique_files):\n",
    "        y, sr = preprocess_audio(raw_f)\n",
    "        if y is not None:\n",
    "            new_name = f\"{folder_name}_{i}.wav\"\n",
    "            full_save_path = os.path.join(save_path, new_name)\n",
    "            sf.write(full_save_path, y, sr)\n",
    "            path_map[raw_f] = os.path.join(folder_name, new_name)\n",
    "\n",
    "        if (i+1) % 400 == 0: print(f\"  Saved {i+1} file...\")\n",
    "\n",
    "    df['file_path'] = df['raw_path'].map(path_map)\n",
    "    return df.dropna(subset=['file_path'])\n",
    "\n",
    "df_train_final = process_and_save_dataset(df_train_balanced, \"train\")\n",
    "df_val_final = process_and_save_dataset(df_val, \"val\")\n",
    "df_test_final = process_and_save_dataset(df_test, \"test\")\n",
    "\n",
    "cols = ['file_path', 'label_id']\n",
    "df_train_final[cols].to_csv(os.path.join(OUTPUT_DIR, \"train_final.csv\"), index=False)\n",
    "df_val_final[cols].to_csv(os.path.join(OUTPUT_DIR, \"val_final.csv\"), index=False)\n",
    "df_test_final[cols].to_csv(os.path.join(OUTPUT_DIR, \"test_final.csv\"), index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"Done!\")\n",
    "print(f\"Final rows:\")\n",
    "print(f\"   - Train (Balanced): {len(df_train_final)}\")\n",
    "print(f\"   - Val: {len(df_val_final)}\")\n",
    "print(f\"   - Test: {len(df_test_final)}\")\n",
    "print(\"=\"*40)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPbB8Q+DvC4E8umDBzt0Tn7",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
