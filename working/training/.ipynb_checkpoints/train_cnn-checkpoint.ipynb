{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T16:13:00.331513Z",
     "iopub.status.busy": "2025-10-26T16:13:00.330858Z",
     "iopub.status.idle": "2025-10-26T16:14:08.797986Z",
     "shell.execute_reply": "2025-10-26T16:14:08.797306Z",
     "shell.execute_reply.started": "2025-10-26T16:13:00.331490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tổng số file: 2880\n",
      "Số file train: 1843, val: 461, test: 576\n",
      "✅ Hoàn tất preprocess, CSV sẵn sàng train mô hình\n",
      "Bảng label -> label_id: {'neutral': 0, 'calm': 1, 'happy': 2, 'sad': 3, 'angry': 4, 'fearful': 5, 'disgust': 6, 'surprised': 7}\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 1_preprocess_emotionid.py\n",
    "# =========================\n",
    "import os\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# =========================\n",
    "# 1. Cấu hình\n",
    "# =========================\n",
    "DATA_DIR = \"/kaggle/input/ravdess-emotional-speech-audio\"  # dataset gốc\n",
    "OUTPUT_DIR = \"/kaggle/working/processed\"  # thư mục lưu WAV + CSV\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Mapping code -> nhãn text\n",
    "EMOTION_MAP = {\n",
    "    '01': 'neutral',\n",
    "    '02': 'calm',\n",
    "    '03': 'happy',\n",
    "    '04': 'sad',\n",
    "    '05': 'angry',\n",
    "    '06': 'fearful',\n",
    "    '07': 'disgust',\n",
    "    '08': 'surprised'\n",
    "}\n",
    "\n",
    "# Mapping EmotionID -> label_id theo gốc (0-index)\n",
    "EMOTION_ID_TO_NUM = {k: int(k)-1 for k in EMOTION_MAP.keys()}\n",
    "# '01' -> 0, '02' -> 1, ..., '08' -> 7\n",
    "\n",
    "# =========================\n",
    "# 2. Hàm preprocess audio\n",
    "# =========================\n",
    "def preprocess_audio(file_path, target_sr=16000):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "        if y.ndim > 1:\n",
    "            y = librosa.to_mono(y)\n",
    "        y_resampled = librosa.resample(y, orig_sr=sr, target_sr=target_sr)\n",
    "        return y_resampled, target_sr\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi xử lý {file_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# =========================\n",
    "# 3. Duyệt file + lấy nhãn\n",
    "# =========================\n",
    "all_files = []\n",
    "all_labels = []\n",
    "all_label_ids = []\n",
    "\n",
    "for root, _, files in os.walk(DATA_DIR):\n",
    "    for f in files:\n",
    "        if f.endswith(\".wav\"):\n",
    "            path = os.path.join(root, f)\n",
    "            try:\n",
    "                code = f.split(\"-\")[2]           # EmotionID từ filename\n",
    "                label_text = EMOTION_MAP[code]   # nhãn text\n",
    "                label_id = EMOTION_ID_TO_NUM[code]  # label_id theo gốc\n",
    "            except:\n",
    "                print(f\"File không chuẩn: {f}\")\n",
    "                continue\n",
    "            all_files.append(path)\n",
    "            all_labels.append(label_text)\n",
    "            all_label_ids.append(label_id)\n",
    "\n",
    "print(f\"Tổng số file: {len(all_files)}\")\n",
    "\n",
    "# =========================\n",
    "# 4. Chia train/test\n",
    "# =========================\n",
    "train_files, test_files, train_labels, test_labels, train_label_ids, test_label_ids = train_test_split(\n",
    "    all_files, all_labels, all_label_ids,\n",
    "    test_size=0.2, stratify=all_label_ids, random_state=42\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 5. Hàm lưu WAV + trả paths\n",
    "# =========================\n",
    "def save_wav(files, prefix):\n",
    "    paths = []\n",
    "    out_dir = os.path.join(OUTPUT_DIR, prefix)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    for i, path in enumerate(files):\n",
    "        y, sr = preprocess_audio(path)\n",
    "        if y is None:\n",
    "            continue\n",
    "        out_name = f\"{prefix}_{i}.wav\"\n",
    "        out_path = os.path.join(out_dir, out_name)\n",
    "        sf.write(out_path, y, sr)\n",
    "        paths.append(out_path)\n",
    "    return paths\n",
    "\n",
    "train_paths = save_wav(train_files, \"train\")\n",
    "test_paths = save_wav(test_files, \"test\")\n",
    "\n",
    "# =========================\n",
    "# 6. Lưu CSV final\n",
    "# =========================\n",
    "train_df = pd.DataFrame({\n",
    "    \"file_path\": train_paths,\n",
    "    \"label\": train_labels,\n",
    "    \"label_id\": train_label_ids\n",
    "})\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    \"file_path\": test_paths,\n",
    "    \"label\": test_labels,\n",
    "    \"label_id\": test_label_ids\n",
    "})\n",
    "\n",
    "# Chia train -> validation\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df, test_size=0.2, stratify=train_df[\"label_id\"], random_state=42\n",
    ")\n",
    "\n",
    "# Lưu CSV\n",
    "train_df.to_csv(os.path.join(OUTPUT_DIR, \"train_final.csv\"), index=False)\n",
    "val_df.to_csv(os.path.join(OUTPUT_DIR, \"val_final.csv\"), index=False)\n",
    "test_df.to_csv(os.path.join(OUTPUT_DIR, \"test_final.csv\"), index=False)\n",
    "\n",
    "print(f\"Số file train: {len(train_df)}, val: {len(val_df)}, test: {len(test_df)}\")\n",
    "print(\"✅ Hoàn tất preprocess, CSV sẵn sàng train mô hình\")\n",
    "print(\"Bảng label -> label_id:\", dict(zip(EMOTION_MAP.values(), EMOTION_ID_TO_NUM.values())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "  Downloading librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Downloading audioread-3.1.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: numpy>=1.22.3 in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from librosa) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from librosa) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from librosa) (1.5.1)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Collecting soundfile>=0.12.1 (from librosa)\n",
      "  Downloading soundfile-0.13.1-py2.py3-none-win_amd64.whl.metadata (16 kB)\n",
      "Collecting pooch>=1.1 (from librosa)\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa)\n",
      "  Downloading soxr-1.0.0-cp312-abi3-win_amd64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from librosa) (4.11.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from librosa) (1.0.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from lazy_loader>=0.1->librosa) (24.1)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from pooch>=1.1->librosa) (3.10.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from scikit-learn>=1.1.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.1.31)\n",
      "Downloading librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "Downloading audioread-3.1.0-py3-none-any.whl (23 kB)\n",
      "Downloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "Downloading soundfile-0.13.1-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.3/1.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.3/1.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.3/1.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.3/1.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.3/1.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.3/1.0 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 0.5/1.0 MB 186.4 kB/s eta 0:00:03\n",
      "   -------------------- ------------------- 0.5/1.0 MB 186.4 kB/s eta 0:00:03\n",
      "   ------------------------------ --------- 0.8/1.0 MB 294.4 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 0.8/1.0 MB 294.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.0/1.0 MB 351.1 kB/s eta 0:00:00\n",
      "Downloading soxr-1.0.0-cp312-abi3-win_amd64.whl (172 kB)\n",
      "Installing collected packages: soxr, audioread, soundfile, pooch, librosa\n",
      "Successfully installed audioread-3.1.0 librosa-0.11.0 pooch-1.8.2 soundfile-0.13.1 soxr-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT_DIR : C:\\Users\\FPT SHOP\\Speech_Emotion_Recognition\\working\n",
      "DATA_DIR : C:\\Users\\FPT SHOP\\Speech_Emotion_Recognition\\working\\processed\n",
      "FEAT_DIR : C:\\Users\\FPT SHOP\\Speech_Emotion_Recognition\\working\\features\n"
     ]
    }
   ],
   "source": [
    "# ----------- 1. Config -----------\n",
    "SR = 16000             #16kHz\n",
    "N_FFT = 1024           # ~64 ms\n",
    "HOP = 256              # ~16 ms\n",
    "WIN = 1024\n",
    "N_MELS = 64\n",
    "N_MFCC = 13\n",
    "\n",
    "MAX_FRAMES = 500 \n",
    "\n",
    "USE_LOGMEL = True\n",
    "USE_MFCC39 = True\n",
    "USE_CHROMA = True\n",
    "#DATA_DIR = Path(\"/kaggle/working/processed\")\n",
    "CURRENT_DIR = Path().resolve()\n",
    "ROOT_DIR = CURRENT_DIR.parent\n",
    "DATA_DIR = ROOT_DIR / \"processed\"\n",
    "FEAT_DIR = ROOT_DIR / \"features\"\n",
    "FEAT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"ROOT_DIR :\", ROOT_DIR)\n",
    "print(\"DATA_DIR :\", DATA_DIR)\n",
    "print(\"FEAT_DIR :\", FEAT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- 2. Utils -----------\n",
    "def load_wav(path, sr = SR):\n",
    "    # đọc wav, resample về SR,chuẩn hóa biên độ [-1, 1]\n",
    "    y, _ = librosa.load(Path(path), sr=sr, mono=True)\n",
    "    if np.max(np.abs(y)) > 0:\n",
    "        y = y / np.max(np.abs(y))\n",
    "    return y\n",
    "#y:  mảng NumPy 1 chiều chứa các giá trị biên độ của sóng âm theo thời gian\n",
    "def extract_logmel(y, sr = SR):\n",
    "    S = np.abs(librosa.stft(y, n_fft=N_FFT, hop_length=HOP, win_length=WIN))**2\n",
    "    M = librosa.feature.melspectrogram(S=S, sr=sr, n_mels=N_MELS)\n",
    "    logmel = librosa.power_to_db(M, ref=np.max)\n",
    "    return logmel\n",
    "#Ma trận 2D logmel có kích thước (N_MELS, T), \n",
    "#trong đó N_MELS là số lượng dải Mel, T là số khung thời gian. \n",
    "def extract_mfcc_block(y, sr = SR):\n",
    "    M = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=N_FFT,\n",
    "                                       hop_length=HOP, win_length=WIN,\n",
    "                                       n_mels=N_MELS)\n",
    "    db = librosa.power_to_db(M, ref=np.max)\n",
    "    mfcc = librosa.feature.mfcc(S=db, n_mfcc=N_MFCC)\n",
    "    d1 = librosa.feature.delta(mfcc)\n",
    "    d2 = librosa.feature.delta(mfcc, order=2)\n",
    "    return np.concatenate([mfcc, d1, d2], axis=0)\n",
    "#Ma trận 2D có kích thước (3 * N_MFCC, T).\n",
    "def extract_chroma(y, sr=SR):\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr, n_fft=N_FFT, hop_length=HOP, win_length=WIN)\n",
    "    return chroma\n",
    "#Ma trận 2D có kích thước (12, T)\n",
    "def pad_or_trim_feat(X, T_target, pad_value=0.0):\n",
    "    # X: (C, T) -> (C, T_target) bằng cách cắt hoặc đệm 0 ở cuối\n",
    "    C, T = X.shape\n",
    "    if T == T_target:\n",
    "        return X\n",
    "    if T > T_target:\n",
    "        return X[:, :T_target]\n",
    "    pad = np.full((C, T_target - T), pad_value, dtype=X.dtype)\n",
    "    return np.concatenate([X, pad], axis=1)\n",
    "\n",
    "def stack_features(y):\n",
    "    \"\"\"\n",
    "    Ghép các khối đặc trưng theo trục kênh (C):\n",
    "      - log-Mel: (N_MELS, T)\n",
    "      - MFCC39: (39, T)\n",
    "      - Chroma: (12, T)\n",
    "    Trả về: (C, T)\n",
    "    \"\"\"\n",
    "    feats = []\n",
    "    if USE_LOGMEL:\n",
    "        feats.append(extract_logmel(y))\n",
    "    if USE_MFCC39:\n",
    "        feats.append(extract_mfcc_block(y))\n",
    "    if USE_CHROMA:\n",
    "        feats.append(extract_chroma(y))\n",
    "    if len(feats) == 0:\n",
    "        raise ValueError(\"Bạn phải bật ít nhất một đặc trưng (LOGMEL/MFCC/CHROMA).\")\n",
    "    # Khớp T (trường hợp sai lệch 1-2 frame do làm tròn) bằng cách cắt theo T nhỏ nhất\n",
    "    T_min = min(f.shape[1] for f in feats)\n",
    "    feats = [f[:, :T_min] for f in feats]\n",
    "    return np.concatenate(feats, axis=0)\n",
    "\n",
    "def compute_norm_stats(X):\n",
    "    \"\"\"X: (N, C, T) -> tính mean/std theo từng kênh C trên toàn bộ frames.\"\"\"\n",
    "    N, C, T = X.shape\n",
    "    flat = X.transpose(1, 0, 2).reshape(C, N*T)\n",
    "    mean = flat.mean(axis=1)\n",
    "    std = flat.std(axis=1) + 1e-8\n",
    "    return mean, std\n",
    "\n",
    "def apply_norm(X, mean, std):\n",
    "    \"\"\"Pre-emphasis theo kênh: (N, C, T) -> normalized.\"\"\"\n",
    "    return (X - mean[None, :, None]) / std[None, :, None]\n",
    "\n",
    "def read_split(split_name):\n",
    "    df = pd.read_csv(DATA_DIR / f\"{split_name}_final.csv\")\n",
    "    # Đảm bảo đúng kiểu\n",
    "    df[\"file_path\"] = df[\"file_path\"].astype(str)\n",
    "    df[\"label_id\"] = df[\"label_id\"].astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Trích xuất cho 1 split\n",
    "# =========================\n",
    "def extract_split(split_name, T_target=None):\n",
    "    \"\"\"\n",
    "    split_name: 'train' | 'val' | 'test'\n",
    "    T_target: nếu None (thường cho train), sẽ chọn = min(T_max, MAX_FRAMES)\n",
    "    Trả về: X_pad (N,C,T), y (N,), paths (list), T_target (int)\n",
    "    \"\"\"\n",
    "    df = read_split(split_name)\n",
    "    X_list, y_list, paths = [], [], []\n",
    "\n",
    "    for path, label_id in zip(df[\"file_path\"].tolist(), df[\"label_id\"].tolist()):\n",
    "        y = load_wav(path)\n",
    "        F = stack_features(y)           # (C, T)\n",
    "        X_list.append(F)\n",
    "        y_list.append(int(label_id))\n",
    "        paths.append(str(path))\n",
    "\n",
    "    # Chọn số frame mục tiêu\n",
    "    if T_target is None:\n",
    "        T_max = max(x.shape[1] for x in X_list)\n",
    "        T_target = min(T_max, MAX_FRAMES)\n",
    "\n",
    "    # Pad/trim và xếp stack\n",
    "    X_pad = np.stack([pad_or_trim_feat(x, T_target) for x in X_list], axis=0)  # (N, C, T_target)\n",
    "    y = np.array(y_list, dtype=np.int64)\n",
    "\n",
    "    return X_pad, y, paths, T_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FPT SHOP\\anaconda3\\Lib\\site-packages\\paramiko\\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\n",
      "C:\\Users\\FPT SHOP\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from this module in 45.0.0.\n",
      "  \"class\": algorithms.Blowfish,\n",
      "C:\\Users\\FPT SHOP\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\n",
      "C:\\Users\\FPT SHOP\\AppData\\Local\\Temp\\ipykernel_32340\\2167378321.py:4: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, _ = librosa.load(Path(path), sr=sr, mono=True)\n",
      "C:\\Users\\FPT SHOP\\anaconda3\\Lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'processed\\\\train\\\\train_256.wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\librosa\\core\\audio.py:176\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m __soundfile_load(path, offset, duration, dtype)\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m sf\u001b[38;5;241m.\u001b[39mSoundFileRuntimeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\librosa\\core\\audio.py:209\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;66;03m# Otherwise, create the soundfile object\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m     context \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mSoundFile(path)\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context \u001b[38;5;28;01mas\u001b[39;00m sf_desc:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\soundfile.py:690\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[1;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd, compression_level, bitrate_mode)\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[0;32m    689\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[1;32m--> 690\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open(file, mode_int, closefd)\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\soundfile.py:1265\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[1;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[0;32m   1264\u001b[0m     err \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_error(file_ptr)\n\u001b[1;32m-> 1265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError opening \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode_int \u001b[38;5;241m==\u001b[39m _snd\u001b[38;5;241m.\u001b[39mSFM_WRITE:\n\u001b[0;32m   1267\u001b[0m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[0;32m   1268\u001b[0m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[0;32m   1269\u001b[0m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n",
      "\u001b[1;31mLibsndfileError\u001b[0m: Error opening 'processed\\\\train\\\\train_256.wav': System error.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNorm stats saved at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFEAT_DIR\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeta.json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 55\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[25], line 3\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# ---- Train ----\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     X_train, y_train, train_paths, T_target \u001b[38;5;241m=\u001b[39m extract_split(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, T_target\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m     mean, std \u001b[38;5;241m=\u001b[39m compute_norm_stats(X_train)\n\u001b[0;32m      5\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m apply_norm(X_train, mean, std)\n",
      "Cell \u001b[1;32mIn[9], line 13\u001b[0m, in \u001b[0;36mextract_split\u001b[1;34m(split_name, T_target)\u001b[0m\n\u001b[0;32m     10\u001b[0m X_list, y_list, paths \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path, label_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(), df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()):\n\u001b[1;32m---> 13\u001b[0m     y \u001b[38;5;241m=\u001b[39m load_wav(path)\n\u001b[0;32m     14\u001b[0m     F \u001b[38;5;241m=\u001b[39m stack_features(y)           \u001b[38;5;66;03m# (C, T)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     X_list\u001b[38;5;241m.\u001b[39mappend(F)\n",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m, in \u001b[0;36mload_wav\u001b[1;34m(path, sr)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_wav\u001b[39m(path, sr \u001b[38;5;241m=\u001b[39m SR):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# đọc wav, resample về SR,chuẩn hóa biên độ [-1, 1]\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     y, _ \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mload(Path(path), sr\u001b[38;5;241m=\u001b[39msr, mono\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmax(np\u001b[38;5;241m.\u001b[39mabs(y)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m      6\u001b[0m         y \u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(np\u001b[38;5;241m.\u001b[39mabs(y))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\librosa\\core\\audio.py:184\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPurePath)):\n\u001b[0;32m    181\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    182\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPySoundFile failed. Trying audioread instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    183\u001b[0m     )\n\u001b[1;32m--> 184\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m __audioread_load(path, offset, duration, dtype)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\librosa\\util\\decorators.py:63\u001b[0m, in \u001b[0;36mdeprecated.<locals>.__wrapper\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Warn the user, and then proceed.\"\"\"\u001b[39;00m\n\u001b[0;32m     55\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mDeprecated as of librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mIt will be removed in librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     61\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# Would be 2, but the decorator adds a level\u001b[39;00m\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\librosa\\core\\audio.py:240\u001b[0m, in \u001b[0;36m__audioread_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    237\u001b[0m     reader \u001b[38;5;241m=\u001b[39m path\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# If the input was not an audioread object, try to open it\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m     reader \u001b[38;5;241m=\u001b[39m audioread\u001b[38;5;241m.\u001b[39maudio_open(path)\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m reader \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[0;32m    243\u001b[0m     sr_native \u001b[38;5;241m=\u001b[39m input_file\u001b[38;5;241m.\u001b[39msamplerate\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\audioread\\__init__.py:126\u001b[0m, in \u001b[0;36maudio_open\u001b[1;34m(path, backends)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m BackendClass \u001b[38;5;129;01min\u001b[39;00m backends:\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 126\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m BackendClass(path)\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError:\n\u001b[0;32m    128\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\audioread\\rawread.py:59\u001b[0m, in \u001b[0;36mRawAudioFile.__init__\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename):\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m aifc\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'processed\\\\train\\\\train_256.wav'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # ---- Train ----\n",
    "    X_train, y_train, train_paths, T_target = extract_split(\"train\", T_target=None)\n",
    "    mean, std = compute_norm_stats(X_train)\n",
    "    X_train = apply_norm(X_train, mean, std)\n",
    "\n",
    "    # ---- Val/Test: dùng cùng T_target & cùng mean/std của train ----\n",
    "    X_val, y_val, val_paths, _ = extract_split(\"val\", T_target=T_target)\n",
    "    X_test, y_test, test_paths, _ = extract_split(\"test\", T_target=T_target)\n",
    "\n",
    "    X_val  = apply_norm(X_val,  mean, std)\n",
    "    X_test = apply_norm(X_test, mean, std)\n",
    "\n",
    "    # ---- Lưu npy ----\n",
    "    np.save(FEAT_DIR / \"X_train.npy\", X_train)\n",
    "    np.save(FEAT_DIR / \"y_train.npy\", y_train)\n",
    "    np.save(FEAT_DIR / \"X_val.npy\",   X_val)\n",
    "    np.save(FEAT_DIR / \"y_val.npy\",   y_val)\n",
    "    np.save(FEAT_DIR / \"X_test.npy\",  X_test)\n",
    "    np.save(FEAT_DIR / \"y_test.npy\",  y_test)\n",
    "\n",
    "    # ---- Lưu meta ----\n",
    "    channels = X_train.shape[1]\n",
    "    meta = {\n",
    "        \"sr\": SR,\n",
    "        \"n_fft\": N_FFT,\n",
    "        \"hop\": HOP,\n",
    "        \"win\": WIN,\n",
    "        \"n_mels\": N_MELS,\n",
    "        \"n_mfcc\": N_MFCC,\n",
    "        \"use_logmel\": USE_LOGMEL,\n",
    "        \"use_mfcc39\": USE_MFCC39,\n",
    "        \"use_chroma\": USE_CHROMA,\n",
    "        \"channels\": int(channels),\n",
    "        \"frames_target\": int(X_train.shape[2]),\n",
    "        \"max_frames_cap\": MAX_FRAMES,\n",
    "        \"mean\": [float(m) for m in mean],\n",
    "        \"std\":  [float(s) for s in std],\n",
    "        \"train_size\": int(X_train.shape[0]),\n",
    "        \"val_size\": int(X_val.shape[0]),\n",
    "        \"test_size\": int(X_test.shape[0]),\n",
    "        \"train_paths_head\": train_paths[:3],  # để debug nhanh\n",
    "        \"val_paths_head\":   val_paths[:3],\n",
    "        \"test_paths_head\":  test_paths[:3],\n",
    "    }\n",
    "    with open(FEAT_DIR / \"meta.json\", \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    print(\"Done feature extraction.\")\n",
    "    print(f\"Shapes -> X_train {X_train.shape}, X_val {X_val.shape}, X_test {X_test.shape}\")\n",
    "    print(f\"Channels: {channels} (LOGMEL={USE_LOGMEL}, MFCC39={USE_MFCC39}, CHROMA={USE_CHROMA})\")\n",
    "    print(f\"Norm stats saved at: {FEAT_DIR/'meta.json'}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T16:17:33.472671Z",
     "iopub.status.busy": "2025-10-26T16:17:33.472400Z",
     "iopub.status.idle": "2025-10-26T16:17:37.847931Z",
     "shell.execute_reply": "2025-10-26T16:17:37.847302Z",
     "shell.execute_reply.started": "2025-10-26T16:17:33.472651Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (1843, 115, 330) (461, 115, 330) (576, 115, 330)\n",
      "Classes: 8\n"
     ]
    }
   ],
   "source": [
    "# === 4. LOAD DATA ===\n",
    "import numpy as np, os, json, torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "FEATURES_DIR = \"/kaggle/working/processed/features\"  # hoặc đường dẫn bạn lưu\n",
    "OUT_DIR = \"/kaggle/working\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Load đặc trưng và nhãn\n",
    "X_train = np.load(os.path.join(FEATURES_DIR, \"X_train.npy\"))\n",
    "y_train = np.load(os.path.join(FEATURES_DIR, \"y_train.npy\"))\n",
    "X_val   = np.load(os.path.join(FEATURES_DIR, \"X_val.npy\"))\n",
    "y_val   = np.load(os.path.join(FEATURES_DIR, \"y_val.npy\"))\n",
    "X_test  = np.load(os.path.join(FEATURES_DIR, \"X_test.npy\"))\n",
    "y_test  = np.load(os.path.join(FEATURES_DIR, \"y_test.npy\"))\n",
    "\n",
    "with open(os.path.join(FEATURES_DIR, \"meta.json\")) as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "print(\"Shapes:\", X_train.shape, X_val.shape, X_test.shape)\n",
    "print(\"Classes:\", len(set(y_train)))\n",
    "\n",
    "# Reshape cho CNN2D: (N, 1, C, T)\n",
    "X_train = X_train[:, None, :, :].astype(\"float32\")\n",
    "X_val   = X_val[:, None, :, :].astype(\"float32\")\n",
    "X_test  = X_test[:, None, :, :].astype(\"float32\")\n",
    "\n",
    "# Dataloader\n",
    "BATCH_SIZE = 32\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_dl = DataLoader(TensorDataset(torch.tensor(X_train), torch.tensor(y_train)),\n",
    "                      batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dl = DataLoader(TensorDataset(torch.tensor(X_val), torch.tensor(y_val)),\n",
    "                    batch_size=BATCH_SIZE)\n",
    "test_dl = DataLoader(TensorDataset(torch.tensor(X_test), torch.tensor(y_test)),\n",
    "                     batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T16:17:37.849538Z",
     "iopub.status.busy": "2025-10-26T16:17:37.849085Z",
     "iopub.status.idle": "2025-10-26T16:17:38.059632Z",
     "shell.execute_reply": "2025-10-26T16:17:38.059017Z",
     "shell.execute_reply.started": "2025-10-26T16:17:37.849510Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SER_CNN(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(5, 7), stride=(1, 1), padding=(2, 3))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(64, 128, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))\n",
      "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.25, inplace=False)\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Linear(in_features=128, out_features=8, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# === 5. MODEL DEFINITION ===\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SER_CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, (5,7), padding=(2,3)), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2)),\n",
    "            nn.Conv2d(32, 64, (3,5), padding=(1,2)), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2)),\n",
    "            nn.Conv2d(64, 128, (3,5), padding=(1,2)), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 128), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Khởi tạo\n",
    "num_classes = len(set(y_train))\n",
    "model = SER_CNN(num_classes).to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T16:17:41.021701Z",
     "iopub.status.busy": "2025-10-26T16:17:41.021121Z",
     "iopub.status.idle": "2025-10-26T16:17:43.664338Z",
     "shell.execute_reply": "2025-10-26T16:17:43.663565Z",
     "shell.execute_reply.started": "2025-10-26T16:17:41.021679Z"
    }
   },
   "outputs": [],
   "source": [
    "# === 6. TRAINING SETUP ===\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=3)\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    all_true, all_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dataloader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred = model(xb).argmax(1)\n",
    "            all_true.append(yb.cpu().numpy())\n",
    "            all_pred.append(pred.cpu().numpy())\n",
    "    y_true = np.concatenate(all_true); y_pred = np.concatenate(all_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    acc = (y_true == y_pred).mean()\n",
    "    return acc, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T16:17:47.671097Z",
     "iopub.status.busy": "2025-10-26T16:17:47.670674Z",
     "iopub.status.idle": "2025-10-26T16:21:20.368077Z",
     "shell.execute_reply": "2025-10-26T16:21:20.367371Z",
     "shell.execute_reply.started": "2025-10-26T16:17:47.671075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: loss=2.0270  val_acc=0.208  val_f1=0.106\n",
      "Epoch 02: loss=1.9353  val_acc=0.269  val_f1=0.185\n",
      "Epoch 03: loss=1.8284  val_acc=0.345  val_f1=0.244\n",
      "Epoch 04: loss=1.7345  val_acc=0.371  val_f1=0.281\n",
      "Epoch 05: loss=1.6596  val_acc=0.397  val_f1=0.310\n",
      "Epoch 06: loss=1.6005  val_acc=0.230  val_f1=0.142\n",
      "Epoch 07: loss=1.5712  val_acc=0.432  val_f1=0.342\n",
      "Epoch 08: loss=1.5135  val_acc=0.299  val_f1=0.237\n",
      "Epoch 09: loss=1.4921  val_acc=0.141  val_f1=0.043\n",
      "Epoch 10: loss=1.4379  val_acc=0.330  val_f1=0.268\n",
      "Epoch 11: loss=1.3965  val_acc=0.408  val_f1=0.347\n",
      "Epoch 12: loss=1.3670  val_acc=0.221  val_f1=0.156\n",
      "Epoch 13: loss=1.3204  val_acc=0.213  val_f1=0.128\n",
      "Epoch 14: loss=1.3174  val_acc=0.219  val_f1=0.120\n",
      "Epoch 15: loss=1.2788  val_acc=0.425  val_f1=0.351\n",
      "Epoch 16: loss=1.2275  val_acc=0.210  val_f1=0.133\n",
      "Epoch 17: loss=1.1801  val_acc=0.176  val_f1=0.094\n",
      "Epoch 18: loss=1.1399  val_acc=0.443  val_f1=0.403\n",
      "Epoch 19: loss=1.1282  val_acc=0.267  val_f1=0.129\n",
      "Epoch 20: loss=1.1045  val_acc=0.134  val_f1=0.030\n",
      "Epoch 21: loss=1.0835  val_acc=0.254  val_f1=0.124\n",
      "Epoch 22: loss=1.0311  val_acc=0.260  val_f1=0.135\n",
      "Epoch 23: loss=0.9768  val_acc=0.579  val_f1=0.529\n",
      "Epoch 24: loss=0.9335  val_acc=0.171  val_f1=0.097\n",
      "Epoch 25: loss=0.8930  val_acc=0.547  val_f1=0.527\n",
      "Epoch 26: loss=0.8836  val_acc=0.319  val_f1=0.228\n",
      "Epoch 27: loss=0.8753  val_acc=0.297  val_f1=0.237\n",
      "Epoch 28: loss=0.8204  val_acc=0.475  val_f1=0.448\n",
      "⏹ Early stopping.\n",
      "Best val F1: 0.5289169175863404\n"
     ]
    }
   ],
   "source": [
    "# === 7. TRAIN LOOP ===\n",
    "EPOCHS = 30\n",
    "best_f1, patience, left = -1, 5, 5\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_dl:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(xb), yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    train_loss = total_loss / len(train_dl.dataset)\n",
    "    \n",
    "    val_acc, val_f1 = evaluate(model, val_dl)\n",
    "    scheduler.step(val_f1)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}: loss={train_loss:.4f}  val_acc={val_acc:.3f}  val_f1={val_f1:.3f}\")\n",
    "\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1, left = val_f1, patience\n",
    "        torch.save(model.state_dict(), os.path.join(OUT_DIR, \"ser_best.pt\"))\n",
    "    else:\n",
    "        left -= 1\n",
    "        if left == 0:\n",
    "            print(\"⏹ Early stopping.\")\n",
    "            break\n",
    "\n",
    "print(\"Best val F1:\", best_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T16:21:32.923098Z",
     "iopub.status.busy": "2025-10-26T16:21:32.922571Z",
     "iopub.status.idle": "2025-10-26T16:21:33.637017Z",
     "shell.execute_reply": "2025-10-26T16:21:33.636371Z",
     "shell.execute_reply.started": "2025-10-26T16:21:32.923071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Accuracy: 0.5729 | Macro-F1: 0.5341\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4000    0.1053    0.1667        38\n",
      "           1     0.5000    1.0000    0.6667        76\n",
      "           2     0.7667    0.2987    0.4299        77\n",
      "           3     0.2895    0.4286    0.3455        77\n",
      "           4     0.8824    0.7792    0.8276        77\n",
      "           5     0.4839    0.3896    0.4317        77\n",
      "           6     0.7531    0.7922    0.7722        77\n",
      "           7     0.7288    0.5584    0.6324        77\n",
      "\n",
      "    accuracy                         0.5729       576\n",
      "   macro avg     0.6005    0.5440    0.5341       576\n",
      "weighted avg     0.6143    0.5729    0.5587       576\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 4 26  0  8  0  0  0  0]\n",
      " [ 0 76  0  0  0  0  0  0]\n",
      " [ 3  5 23 22  1 12  5  6]\n",
      " [ 2 32  0 33  0  4  6  0]\n",
      " [ 0  0  0  1 60  3  7  6]\n",
      " [ 0  5  1 37  0 30  0  4]\n",
      " [ 0  6  0  7  3  0 61  0]\n",
      " [ 1  2  6  6  4 13  2 43]]\n"
     ]
    }
   ],
   "source": [
    "# === 8. TEST EVALUATION ===\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "model.load_state_dict(torch.load(os.path.join(OUT_DIR, \"ser_best.pt\"), map_location=device))\n",
    "acc, f1 = evaluate(model, test_dl)\n",
    "print(f\"\\n Test Accuracy: {acc:.4f} | Macro-F1: {f1:.4f}\")\n",
    "\n",
    "# In báo cáo chi tiết\n",
    "y_true, y_pred = [], []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_dl:\n",
    "        xb = xb.to(device)\n",
    "        preds = model(xb).argmax(1).cpu().numpy()\n",
    "        y_true.append(yb.numpy()); y_pred.append(preds)\n",
    "y_true = np.concatenate(y_true); y_pred = np.concatenate(y_pred)\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, digits=4))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 107620,
     "sourceId": 256618,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
