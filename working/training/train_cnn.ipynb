{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T16:13:00.331513Z",
     "iopub.status.busy": "2025-10-26T16:13:00.330858Z",
     "iopub.status.idle": "2025-10-26T16:14:08.797986Z",
     "shell.execute_reply": "2025-10-26T16:14:08.797306Z",
     "shell.execute_reply.started": "2025-10-26T16:13:00.331490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tổng số file: 2880\n",
      "Số file train: 1843, val: 461, test: 576\n",
      "✅ Hoàn tất preprocess, CSV sẵn sàng train mô hình\n",
      "Bảng label -> label_id: {'neutral': 0, 'calm': 1, 'happy': 2, 'sad': 3, 'angry': 4, 'fearful': 5, 'disgust': 6, 'surprised': 7}\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 1_preprocess_emotionid.py\n",
    "# =========================\n",
    "import os\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# =========================\n",
    "# 1. Cấu hình\n",
    "# =========================\n",
    "DATA_DIR = \"/kaggle/input/ravdess-emotional-speech-audio\"  # dataset gốc\n",
    "OUTPUT_DIR = \"/kaggle/working/processed\"  # thư mục lưu WAV + CSV\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Mapping code -> nhãn text\n",
    "EMOTION_MAP = {\n",
    "    '01': 'neutral',\n",
    "    '02': 'calm',\n",
    "    '03': 'happy',\n",
    "    '04': 'sad',\n",
    "    '05': 'angry',\n",
    "    '06': 'fearful',\n",
    "    '07': 'disgust',\n",
    "    '08': 'surprised'\n",
    "}\n",
    "\n",
    "# Mapping EmotionID -> label_id theo gốc (0-index)\n",
    "EMOTION_ID_TO_NUM = {k: int(k)-1 for k in EMOTION_MAP.keys()}\n",
    "# '01' -> 0, '02' -> 1, ..., '08' -> 7\n",
    "\n",
    "# =========================\n",
    "# 2. Hàm preprocess audio\n",
    "# =========================\n",
    "def preprocess_audio(file_path, target_sr=16000):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "        if y.ndim > 1:\n",
    "            y = librosa.to_mono(y)\n",
    "        y_resampled = librosa.resample(y, orig_sr=sr, target_sr=target_sr)\n",
    "        return y_resampled, target_sr\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi xử lý {file_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# =========================\n",
    "# 3. Duyệt file + lấy nhãn\n",
    "# =========================\n",
    "all_files = []\n",
    "all_labels = []\n",
    "all_label_ids = []\n",
    "\n",
    "for root, _, files in os.walk(DATA_DIR):\n",
    "    for f in files:\n",
    "        if f.endswith(\".wav\"):\n",
    "            path = os.path.join(root, f)\n",
    "            try:\n",
    "                code = f.split(\"-\")[2]           # EmotionID từ filename\n",
    "                label_text = EMOTION_MAP[code]   # nhãn text\n",
    "                label_id = EMOTION_ID_TO_NUM[code]  # label_id theo gốc\n",
    "            except:\n",
    "                print(f\"File không chuẩn: {f}\")\n",
    "                continue\n",
    "            all_files.append(path)\n",
    "            all_labels.append(label_text)\n",
    "            all_label_ids.append(label_id)\n",
    "\n",
    "print(f\"Tổng số file: {len(all_files)}\")\n",
    "\n",
    "# =========================\n",
    "# 4. Chia train/test\n",
    "# =========================\n",
    "train_files, test_files, train_labels, test_labels, train_label_ids, test_label_ids = train_test_split(\n",
    "    all_files, all_labels, all_label_ids,\n",
    "    test_size=0.2, stratify=all_label_ids, random_state=42\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 5. Hàm lưu WAV + trả paths\n",
    "# =========================\n",
    "def save_wav(files, prefix):\n",
    "    paths = []\n",
    "    out_dir = os.path.join(OUTPUT_DIR, prefix)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    for i, path in enumerate(files):\n",
    "        y, sr = preprocess_audio(path)\n",
    "        if y is None:\n",
    "            continue\n",
    "        out_name = f\"{prefix}_{i}.wav\"\n",
    "        out_path = os.path.join(out_dir, out_name)\n",
    "        sf.write(out_path, y, sr)\n",
    "        paths.append(out_path)\n",
    "    return paths\n",
    "\n",
    "train_paths = save_wav(train_files, \"train\")\n",
    "test_paths = save_wav(test_files, \"test\")\n",
    "\n",
    "# =========================\n",
    "# 6. Lưu CSV final\n",
    "# =========================\n",
    "train_df = pd.DataFrame({\n",
    "    \"file_path\": train_paths,\n",
    "    \"label\": train_labels,\n",
    "    \"label_id\": train_label_ids\n",
    "})\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    \"file_path\": test_paths,\n",
    "    \"label\": test_labels,\n",
    "    \"label_id\": test_label_ids\n",
    "})\n",
    "\n",
    "# Chia train -> validation\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df, test_size=0.2, stratify=train_df[\"label_id\"], random_state=42\n",
    ")\n",
    "\n",
    "# Lưu CSV\n",
    "train_df.to_csv(os.path.join(OUTPUT_DIR, \"train_final.csv\"), index=False)\n",
    "val_df.to_csv(os.path.join(OUTPUT_DIR, \"val_final.csv\"), index=False)\n",
    "test_df.to_csv(os.path.join(OUTPUT_DIR, \"test_final.csv\"), index=False)\n",
    "\n",
    "print(f\"Số file train: {len(train_df)}, val: {len(val_df)}, test: {len(test_df)}\")\n",
    "print(\"✅ Hoàn tất preprocess, CSV sẵn sàng train mô hình\")\n",
    "print(\"Bảng label -> label_id:\", dict(zip(EMOTION_MAP.values(), EMOTION_ID_TO_NUM.values())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "  Downloading librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Downloading audioread-3.1.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: numpy>=1.22.3 in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from librosa) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from librosa) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from librosa) (1.5.1)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Collecting soundfile>=0.12.1 (from librosa)\n",
      "  Downloading soundfile-0.13.1-py2.py3-none-win_amd64.whl.metadata (16 kB)\n",
      "Collecting pooch>=1.1 (from librosa)\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa)\n",
      "  Downloading soxr-1.0.0-cp312-abi3-win_amd64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from librosa) (4.11.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from librosa) (1.0.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from lazy_loader>=0.1->librosa) (24.1)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from pooch>=1.1->librosa) (3.10.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from scikit-learn>=1.1.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fpt shop\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.1.31)\n",
      "Downloading librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "Downloading audioread-3.1.0-py3-none-any.whl (23 kB)\n",
      "Downloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "Downloading soundfile-0.13.1-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.3/1.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.3/1.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.3/1.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.3/1.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.3/1.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.3/1.0 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 0.5/1.0 MB 186.4 kB/s eta 0:00:03\n",
      "   -------------------- ------------------- 0.5/1.0 MB 186.4 kB/s eta 0:00:03\n",
      "   ------------------------------ --------- 0.8/1.0 MB 294.4 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 0.8/1.0 MB 294.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.0/1.0 MB 351.1 kB/s eta 0:00:00\n",
      "Downloading soxr-1.0.0-cp312-abi3-win_amd64.whl (172 kB)\n",
      "Installing collected packages: soxr, audioread, soundfile, pooch, librosa\n",
      "Successfully installed audioread-3.1.0 librosa-0.11.0 pooch-1.8.2 soundfile-0.13.1 soxr-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT_DIR : C:\\Users\\FPT SHOP\\Speech_Emotion_Recognition\\working\n",
      "DATA_DIR : C:\\Users\\FPT SHOP\\Speech_Emotion_Recognition\\working\\processed\n",
      "FEAT_DIR : C:\\Users\\FPT SHOP\\Speech_Emotion_Recognition\\working\\features\n"
     ]
    }
   ],
   "source": [
    "# ----------- 1. Config -----------\n",
    "SR = 16000             #16kHz\n",
    "N_FFT = 1024           # ~64 ms\n",
    "HOP = 256              # ~16 ms\n",
    "WIN = 1024\n",
    "N_MELS = 64\n",
    "N_MFCC = 13\n",
    "\n",
    "MAX_FRAMES = 500 \n",
    "\n",
    "USE_LOGMEL = True\n",
    "USE_MFCC39 = True\n",
    "USE_CHROMA = True\n",
    "#DATA_DIR = Path(\"/kaggle/working/processed\")\n",
    "CURRENT_DIR = Path().resolve()\n",
    "ROOT_DIR = CURRENT_DIR.parent\n",
    "DATA_DIR = ROOT_DIR / \"processed\"\n",
    "FEAT_DIR = ROOT_DIR / \"features\"\n",
    "FEAT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"ROOT_DIR :\", ROOT_DIR)\n",
    "print(\"DATA_DIR :\", DATA_DIR)\n",
    "print(\"FEAT_DIR :\", FEAT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- 2. Utils -----------\n",
    "def load_wav(path, sr = SR):\n",
    "    # đọc wav, resample về SR,chuẩn hóa biên độ [-1, 1]\n",
    "    y, _ = librosa.load(Path(path), sr=sr, mono=True)\n",
    "    if np.max(np.abs(y)) > 0:\n",
    "        y = y / np.max(np.abs(y))\n",
    "    return y\n",
    "#y:  mảng NumPy 1 chiều chứa các giá trị biên độ của sóng âm theo thời gian\n",
    "def extract_logmel(y, sr = SR):\n",
    "    S = np.abs(librosa.stft(y, n_fft=N_FFT, hop_length=HOP, win_length=WIN))**2\n",
    "    M = librosa.feature.melspectrogram(S=S, sr=sr, n_mels=N_MELS)\n",
    "    logmel = librosa.power_to_db(M, ref=np.max)\n",
    "    return logmel\n",
    "#Ma trận 2D logmel có kích thước (N_MELS, T), \n",
    "#trong đó N_MELS là số lượng dải Mel, T là số khung thời gian. \n",
    "def extract_mfcc_block(y, sr = SR):\n",
    "    M = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=N_FFT,\n",
    "                                       hop_length=HOP, win_length=WIN,\n",
    "                                       n_mels=N_MELS)\n",
    "    db = librosa.power_to_db(M, ref=np.max)\n",
    "    mfcc = librosa.feature.mfcc(S=db, n_mfcc=N_MFCC)\n",
    "    d1 = librosa.feature.delta(mfcc)\n",
    "    d2 = librosa.feature.delta(mfcc, order=2)\n",
    "    return np.concatenate([mfcc, d1, d2], axis=0)\n",
    "#Ma trận 2D có kích thước (3 * N_MFCC, T).\n",
    "def extract_chroma(y, sr=SR):\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr, n_fft=N_FFT, hop_length=HOP, win_length=WIN)\n",
    "    return chroma\n",
    "#Ma trận 2D có kích thước (12, T)\n",
    "def pad_or_trim_feat(X, T_target, pad_value=0.0):\n",
    "    # X: (C, T) -> (C, T_target) bằng cách cắt hoặc đệm 0 ở cuối\n",
    "    C, T = X.shape\n",
    "    if T == T_target:\n",
    "        return X\n",
    "    if T > T_target:\n",
    "        return X[:, :T_target]\n",
    "    pad = np.full((C, T_target - T), pad_value, dtype=X.dtype)\n",
    "    return np.concatenate([X, pad], axis=1)\n",
    "\n",
    "def stack_features(y):\n",
    "    \"\"\"\n",
    "    Ghép các khối đặc trưng theo trục kênh (C):\n",
    "      - log-Mel: (N_MELS, T)\n",
    "      - MFCC39: (39, T)\n",
    "      - Chroma: (12, T)\n",
    "    Trả về: (C, T)\n",
    "    \"\"\"\n",
    "    feats = []\n",
    "    if USE_LOGMEL:\n",
    "        feats.append(extract_logmel(y))\n",
    "    if USE_MFCC39:\n",
    "        feats.append(extract_mfcc_block(y))\n",
    "    if USE_CHROMA:\n",
    "        feats.append(extract_chroma(y))\n",
    "    if len(feats) == 0:\n",
    "        raise ValueError(\"Bạn phải bật ít nhất một đặc trưng (LOGMEL/MFCC/CHROMA).\")\n",
    "    # Khớp T (trường hợp sai lệch 1-2 frame do làm tròn) bằng cách cắt theo T nhỏ nhất\n",
    "    T_min = min(f.shape[1] for f in feats)\n",
    "    feats = [f[:, :T_min] for f in feats]\n",
    "    return np.concatenate(feats, axis=0)\n",
    "\n",
    "def compute_norm_stats(X):\n",
    "    \"\"\"X: (N, C, T) -> tính mean/std theo từng kênh C trên toàn bộ frames.\"\"\"\n",
    "    N, C, T = X.shape\n",
    "    flat = X.transpose(1, 0, 2).reshape(C, N*T)\n",
    "    mean = flat.mean(axis=1)\n",
    "    std = flat.std(axis=1) + 1e-8\n",
    "    return mean, std\n",
    "\n",
    "def apply_norm(X, mean, std):\n",
    "    \"\"\"Pre-emphasis theo kênh: (N, C, T) -> normalized.\"\"\"\n",
    "    return (X - mean[None, :, None]) / std[None, :, None]\n",
    "\n",
    "def read_split(split_name):\n",
    "    df = pd.read_csv(DATA_DIR / f\"{split_name}_final.csv\")\n",
    "    # Đảm bảo đúng kiểu\n",
    "    df[\"file_path\"] = df[\"file_path\"].astype(str)\n",
    "    df[\"label_id\"] = df[\"label_id\"].astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Trích xuất cho 1 split\n",
    "# =========================\n",
    "def extract_split(split_name, T_target=None):\n",
    "    \"\"\"\n",
    "    split_name: 'train' | 'val' | 'test'\n",
    "    T_target: nếu None (thường cho train), sẽ chọn = min(T_max, MAX_FRAMES)\n",
    "    Trả về: X_pad (N,C,T), y (N,), paths (list), T_target (int)\n",
    "    \"\"\"\n",
    "    df = read_split(split_name)\n",
    "    X_list, y_list, paths = [], [], []\n",
    "\n",
    "    for path, label_id in zip(df[\"file_path\"].tolist(), df[\"label_id\"].tolist()):\n",
    "        path = ROOT_DIR/path;\n",
    "        y = load_wav(path)\n",
    "        F = stack_features(y)           # (C, T)\n",
    "        X_list.append(F)\n",
    "        y_list.append(int(label_id))\n",
    "        paths.append(str(path))\n",
    "\n",
    "    # Chọn số frame mục tiêu\n",
    "    if T_target is None:\n",
    "        T_max = max(x.shape[1] for x in X_list)\n",
    "        T_target = min(T_max, MAX_FRAMES)\n",
    "\n",
    "    # Pad/trim và xếp stack\n",
    "    X_pad = np.stack([pad_or_trim_feat(x, T_target) for x in X_list], axis=0)  # (N, C, T_target)\n",
    "    y = np.array(y_list, dtype=np.int64)\n",
    "\n",
    "    return X_pad, y, paths, T_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FPT SHOP\\anaconda3\\Lib\\site-packages\\paramiko\\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\n",
      "C:\\Users\\FPT SHOP\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from this module in 45.0.0.\n",
      "  \"class\": algorithms.Blowfish,\n",
      "C:\\Users\\FPT SHOP\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done feature extraction.\n",
      "Shapes -> X_train (921, 115, 330), X_val (231, 115, 330), X_test (288, 115, 330)\n",
      "Channels: 115 (LOGMEL=True, MFCC39=True, CHROMA=True)\n",
      "Norm stats saved at: C:\\Users\\FPT SHOP\\Speech_Emotion_Recognition\\working\\features\\meta.json\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # ---- Train ----\n",
    "    X_train, y_train, train_paths, T_target = extract_split(\"train\", T_target=None)\n",
    "    mean, std = compute_norm_stats(X_train)\n",
    "    X_train = apply_norm(X_train, mean, std)\n",
    "\n",
    "    # ---- Val/Test: dùng cùng T_target & cùng mean/std của train ----\n",
    "    X_val, y_val, val_paths, _ = extract_split(\"val\", T_target=T_target)\n",
    "    X_test, y_test, test_paths, _ = extract_split(\"test\", T_target=T_target)\n",
    "\n",
    "    X_val  = apply_norm(X_val,  mean, std)\n",
    "    X_test = apply_norm(X_test, mean, std)\n",
    "\n",
    "    # ---- Lưu npy ----\n",
    "    np.save(FEAT_DIR / \"X_train.npy\", X_train)\n",
    "    np.save(FEAT_DIR / \"y_train.npy\", y_train)\n",
    "    np.save(FEAT_DIR / \"X_val.npy\",   X_val)\n",
    "    np.save(FEAT_DIR / \"y_val.npy\",   y_val)\n",
    "    np.save(FEAT_DIR / \"X_test.npy\",  X_test)\n",
    "    np.save(FEAT_DIR / \"y_test.npy\",  y_test)\n",
    "\n",
    "    # ---- Lưu meta ----\n",
    "    channels = X_train.shape[1]\n",
    "    meta = {\n",
    "        \"sr\": SR,\n",
    "        \"n_fft\": N_FFT,\n",
    "        \"hop\": HOP,\n",
    "        \"win\": WIN,\n",
    "        \"n_mels\": N_MELS,\n",
    "        \"n_mfcc\": N_MFCC,\n",
    "        \"use_logmel\": USE_LOGMEL,\n",
    "        \"use_mfcc39\": USE_MFCC39,\n",
    "        \"use_chroma\": USE_CHROMA,\n",
    "        \"channels\": int(channels),\n",
    "        \"frames_target\": int(X_train.shape[2]),\n",
    "        \"max_frames_cap\": MAX_FRAMES,\n",
    "        \"mean\": [float(m) for m in mean],\n",
    "        \"std\":  [float(s) for s in std],\n",
    "        \"train_size\": int(X_train.shape[0]),\n",
    "        \"val_size\": int(X_val.shape[0]),\n",
    "        \"test_size\": int(X_test.shape[0]),\n",
    "        \"train_paths_head\": train_paths[:3],  # để debug nhanh\n",
    "        \"val_paths_head\":   val_paths[:3],\n",
    "        \"test_paths_head\":  test_paths[:3],\n",
    "    }\n",
    "    with open(FEAT_DIR / \"meta.json\", \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    print(\"Done feature extraction.\")\n",
    "    print(f\"Shapes -> X_train {X_train.shape}, X_val {X_val.shape}, X_test {X_test.shape}\")\n",
    "    print(f\"Channels: {channels} (LOGMEL={USE_LOGMEL}, MFCC39={USE_MFCC39}, CHROMA={USE_CHROMA})\")\n",
    "    print(f\"Norm stats saved at: {FEAT_DIR/'meta.json'}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (921, 115, 330) (231, 115, 330) (288, 115, 330)\n",
      "Classes: 8\n"
     ]
    }
   ],
   "source": [
    "# === 4. LOAD DATA ===\n",
    "import numpy as np, os, json, torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "#FEATURES_DIR = \"/kaggle/working/processed/features\"  # hoặc đường dẫn bạn lưu\n",
    "FEATURES_DIR = FEAT_DIR\n",
    "#OUT_DIR = \"/kaggle/working\"\n",
    "OUT_DIR = ROOT_DIR\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Load đặc trưng và nhãn\n",
    "X_train = np.load(os.path.join(FEATURES_DIR, \"X_train.npy\"))\n",
    "y_train = np.load(os.path.join(FEATURES_DIR, \"y_train.npy\"))\n",
    "X_val   = np.load(os.path.join(FEATURES_DIR, \"X_val.npy\"))\n",
    "y_val   = np.load(os.path.join(FEATURES_DIR, \"y_val.npy\"))\n",
    "X_test  = np.load(os.path.join(FEATURES_DIR, \"X_test.npy\"))\n",
    "y_test  = np.load(os.path.join(FEATURES_DIR, \"y_test.npy\"))\n",
    "\n",
    "with open(os.path.join(FEATURES_DIR, \"meta.json\")) as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "print(\"Shapes:\", X_train.shape, X_val.shape, X_test.shape)\n",
    "print(\"Classes:\", len(set(y_train)))\n",
    "\n",
    "# Reshape cho CNN2D: (N, 1, C, T)\n",
    "X_train = X_train[:, None, :, :].astype(\"float32\")\n",
    "X_val   = X_val[:, None, :, :].astype(\"float32\")\n",
    "X_test  = X_test[:, None, :, :].astype(\"float32\")\n",
    "\n",
    "# Dataloader\n",
    "BATCH_SIZE = 32\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_dl = DataLoader(TensorDataset(torch.tensor(X_train), torch.tensor(y_train)),\n",
    "                      batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dl = DataLoader(TensorDataset(torch.tensor(X_val), torch.tensor(y_val)),\n",
    "                    batch_size=BATCH_SIZE)\n",
    "test_dl = DataLoader(TensorDataset(torch.tensor(X_test), torch.tensor(y_test)),\n",
    "                     batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SER_CNN(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(5, 7), stride=(1, 1), padding=(2, 3))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(64, 128, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))\n",
      "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.25, inplace=False)\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Linear(in_features=128, out_features=8, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# === 5. MODEL DEFINITION ===\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SER_CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, (5,7), padding=(2,3)), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2)),\n",
    "            nn.Conv2d(32, 64, (3,5), padding=(1,2)), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2)),\n",
    "            nn.Conv2d(64, 128, (3,5), padding=(1,2)), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 128), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Khởi tạo\n",
    "num_classes = len(set(y_train))\n",
    "model = SER_CNN(num_classes).to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 6. TRAINING SETUP ===\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=3)\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    all_true, all_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dataloader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred = model(xb).argmax(1)\n",
    "            all_true.append(yb.cpu().numpy())\n",
    "            all_pred.append(pred.cpu().numpy())\n",
    "    y_true = np.concatenate(all_true); y_pred = np.concatenate(all_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    acc = (y_true == y_pred).mean()\n",
    "    return acc, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: loss=1.5162  val_acc=0.182  val_f1=0.093\n",
      "Epoch 02: loss=1.4932  val_acc=0.403  val_f1=0.341\n",
      "Epoch 03: loss=1.4761  val_acc=0.255  val_f1=0.163\n",
      "Epoch 04: loss=1.5102  val_acc=0.255  val_f1=0.162\n",
      "Epoch 05: loss=1.4531  val_acc=0.433  val_f1=0.364\n",
      "Epoch 06: loss=1.4194  val_acc=0.463  val_f1=0.416\n",
      "Epoch 07: loss=1.4189  val_acc=0.377  val_f1=0.315\n",
      "Epoch 08: loss=1.4282  val_acc=0.247  val_f1=0.130\n",
      "Epoch 09: loss=1.3907  val_acc=0.489  val_f1=0.411\n",
      "Epoch 10: loss=1.3790  val_acc=0.247  val_f1=0.139\n",
      "Epoch 11: loss=1.3852  val_acc=0.394  val_f1=0.335\n",
      "⏹ Early stopping.\n",
      "Best val F1: 0.41582228864686854\n"
     ]
    }
   ],
   "source": [
    "# === 7. TRAIN LOOP ===\n",
    "EPOCHS = 30\n",
    "best_f1, patience, left = -1, 5, 5\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_dl:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(xb), yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    train_loss = total_loss / len(train_dl.dataset)\n",
    "    \n",
    "    val_acc, val_f1 = evaluate(model, val_dl)\n",
    "    scheduler.step(val_f1)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}: loss={train_loss:.4f}  val_acc={val_acc:.3f}  val_f1={val_f1:.3f}\")\n",
    "\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1, left = val_f1, patience\n",
    "        torch.save(model.state_dict(), os.path.join(OUT_DIR,\"models\", \"ser_best.pt\"))\n",
    "    else:\n",
    "        left -= 1\n",
    "        if left == 0:\n",
    "            print(\"⏹ Early stopping.\")\n",
    "            break\n",
    "\n",
    "print(\"Best val F1:\", best_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Accuracy: 0.4271 | Macro-F1: 0.3714\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000        19\n",
      "           1     0.3895    0.9737    0.5564        38\n",
      "           2     0.3529    0.1579    0.2182        38\n",
      "           3     0.1613    0.1316    0.1449        38\n",
      "           4     0.6667    0.5128    0.5797        39\n",
      "           5     0.3953    0.4359    0.4146        39\n",
      "           6     0.4783    0.5789    0.5238        38\n",
      "           7     0.7619    0.4103    0.5333        39\n",
      "\n",
      "    accuracy                         0.4271       288\n",
      "   macro avg     0.4007    0.4001    0.3714       288\n",
      "weighted avg     0.4293    0.4271    0.3973       288\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 0 13  0  6  0  0  0  0]\n",
      " [ 1 37  0  0  0  0  0  0]\n",
      " [ 0  5  6  8  5 10  3  1]\n",
      " [ 0 24  1  5  0  5  3  0]\n",
      " [ 1  1  5  0 20  2  8  2]\n",
      " [ 2  7  2  3  1 17  5  2]\n",
      " [ 0  7  1  7  1  0 22  0]\n",
      " [ 1  1  2  2  3  9  5 16]]\n"
     ]
    }
   ],
   "source": [
    "# === 8. TEST EVALUATION ===\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "model.load_state_dict(torch.load(os.path.join(OUT_DIR,\"models\", \"ser_best.pt\"), map_location=device))\n",
    "acc, f1 = evaluate(model, test_dl)\n",
    "print(f\"\\n Test Accuracy: {acc:.4f} | Macro-F1: {f1:.4f}\")\n",
    "\n",
    "# In báo cáo chi tiết\n",
    "y_true, y_pred = [], []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_dl:\n",
    "        xb = xb.to(device)\n",
    "        preds = model(xb).argmax(1).cpu().numpy()\n",
    "        y_true.append(yb.numpy()); y_pred.append(preds)\n",
    "y_true = np.concatenate(y_true); y_pred = np.concatenate(y_pred)\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, digits=4))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 107620,
     "sourceId": 256618,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
