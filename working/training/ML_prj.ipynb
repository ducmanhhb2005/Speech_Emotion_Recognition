{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtR8KM-Ji07y"
      },
      "source": [
        "\n",
        "#**Xây dựng mô hình CNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-DuzIk2gt3p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIr5qL7rhiIP"
      },
      "outputs": [],
      "source": [
        "#set up parameter\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BASE = \"/content/drive/MyDrive/Speech_Emotion_Recognition/working/features\"\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 30\n",
        "LR = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hkyae6p_hl2Y"
      },
      "outputs": [],
      "source": [
        "class SpeechEmotionDataset(Dataset):\n",
        "    def __init__(self, split):\n",
        "        base_dir = \"/content/drive/MyDrive/Speech_Emotion_Recognition/working/features\"\n",
        "        split_dir = os.path.join(base_dir, split)\n",
        "\n",
        "        def load_feature(path):\n",
        "            arr = np.load(path, allow_pickle=True)\n",
        "            fixed = []\n",
        "            for x in arr:\n",
        "                if isinstance(x, np.ndarray):\n",
        "                    fixed.append(x.astype(np.float32))\n",
        "                elif isinstance(x, list):\n",
        "                    fixed.append(np.array(x, dtype=np.float32))\n",
        "                else:                    \n",
        "                    continue\n",
        "            return fixed\n",
        "\n",
        "        self.mel = load_feature(os.path.join(split_dir, \"mel.npy\"))\n",
        "        self.mfcc = load_feature(os.path.join(split_dir, \"mfcc.npy\"))\n",
        "        self.chroma = load_feature(os.path.join(split_dir, \"chroma.npy\"))\n",
        "        self.labels = np.load(os.path.join(split_dir, \"labels.npy\"), allow_pickle=True)\n",
        "\n",
        "        #shape chuẩn\n",
        "        self.mel_shape = (128, 128)\n",
        "        self.mfcc_shape = (13, 128)\n",
        "        self.chroma_shape = (12, 128)\n",
        "\n",
        "    def pad_or_trim(self, x, target_shape):\n",
        "        if not isinstance(x, np.ndarray):\n",
        "            x = np.array(x, dtype=np.float32)\n",
        "\n",
        "        h, w = x.shape\n",
        "        H, W = target_shape\n",
        "\n",
        "        if w < W:\n",
        "            pad_w = ((0, 0), (0, W - w))\n",
        "            x = np.pad(x, pad_w, mode='constant')\n",
        "        else:\n",
        "            x = x[:, :W]\n",
        "\n",
        "        if h < H:\n",
        "            pad_h = ((0, H - h), (0, 0))\n",
        "            x = np.pad(x, pad_h, mode='constant')\n",
        "        else:\n",
        "            x = x[:H, :]\n",
        "\n",
        "        return x.astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        mel = self.pad_or_trim(self.mel[idx], self.mel_shape)\n",
        "        mfcc = self.pad_or_trim(self.mfcc[idx], self.mfcc_shape)\n",
        "        chroma = self.pad_or_trim(self.chroma[idx], self.chroma_shape)\n",
        "\n",
        "        mel = torch.tensor(mel).unsqueeze(0)       # (1, 128, 128)\n",
        "        mfcc = torch.tensor(mfcc).unsqueeze(0)     # (1, 13, 128)\n",
        "        chroma = torch.tensor(chroma).unsqueeze(0) # (1, 12, 128)\n",
        "        label = torch.tensor(self.labels[idx]).long()\n",
        "\n",
        "        return mel, mfcc, chroma, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fJSnvMXrBFk",
        "outputId": "59e63001-ddf8-4b3c-e4f5-52be94993a29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "921\n",
            "torch.Size([1, 128, 128]) torch.Size([1, 13, 128]) torch.Size([1, 12, 128]) tensor(4)\n"
          ]
        }
      ],
      "source": [
        "ds = SpeechEmotionDataset(\"train\")\n",
        "print(len(ds))\n",
        "mel, mfcc, chroma, label = ds[0]\n",
        "print(mel.shape, mfcc.shape, chroma.shape, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OHIwoGb3iNgr",
        "outputId": "ebd58f91-d9a9-4bc6-cf63-88a937d04841"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classes: 8\n",
            "MultiFeatureCNN(\n",
            "  (branch_mel): Sequential(\n",
            "    (0): Sequential(\n",
            "      (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (4): Dropout(p=0.3, inplace=False)\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (4): Dropout(p=0.3, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (branch_mfcc): Sequential(\n",
            "    (0): Sequential(\n",
            "      (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (4): Dropout(p=0.3, inplace=False)\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (4): Dropout(p=0.3, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (branch_chroma): Sequential(\n",
            "    (0): Sequential(\n",
            "      (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (4): Dropout(p=0.3, inplace=False)\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (4): Dropout(p=0.3, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=38912, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.4, inplace=False)\n",
            "    (3): Linear(in_features=128, out_features=8, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "#model CNN\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultiFeatureCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(MultiFeatureCNN, self).__init__()\n",
        "\n",
        "        def conv_block(in_c, out_c):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(out_c),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(2),\n",
        "                nn.Dropout(0.3)\n",
        "            )\n",
        "\n",
        "        # 3 branch CNN cho MFCC / Mel / Chroma\n",
        "        self.branch_mel = nn.Sequential(conv_block(1, 16), conv_block(16, 32))\n",
        "        self.branch_mfcc = nn.Sequential(conv_block(1, 16), conv_block(16, 32))\n",
        "        self.branch_chroma = nn.Sequential(conv_block(1, 16), conv_block(16, 32))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            dummy_mel = torch.zeros(1, 1, 128, 128)\n",
        "            dummy_mfcc = torch.zeros(1, 1, 13, 128)\n",
        "            dummy_chroma = torch.zeros(1, 1, 12, 128)\n",
        "\n",
        "            mel_out = self.branch_mel(dummy_mel)\n",
        "            mfcc_out = self.branch_mfcc(dummy_mfcc)\n",
        "            chroma_out = self.branch_chroma(dummy_chroma)\n",
        "\n",
        "            mel_dim = mel_out.numel() // mel_out.shape[0]\n",
        "            mfcc_dim = mfcc_out.numel() // mfcc_out.shape[0]\n",
        "            chroma_dim = chroma_out.numel() // chroma_out.shape[0]\n",
        "            total_dim = mel_dim + mfcc_dim + chroma_dim\n",
        "\n",
        "        #fully connected\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(total_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, mel, mfcc, chroma):\n",
        "        f1 = self.branch_mel(mel)\n",
        "        f2 = self.branch_mfcc(mfcc)\n",
        "        f3 = self.branch_chroma(chroma)\n",
        "\n",
        "        f1 = f1.flatten(1)\n",
        "        f2 = f2.flatten(1)\n",
        "        f3 = f3.flatten(1)\n",
        "\n",
        "        x = torch.cat((f1, f2, f3), dim=1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "#example\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dataset = SpeechEmotionDataset(\"train\")\n",
        "val_dataset = SpeechEmotionDataset(\"val\")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "num_classes = len(np.unique(train_dataset.labels))\n",
        "print(\"Classes:\", num_classes)\n",
        "\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = len(np.unique(train_dataset.labels))\n",
        "model = MultiFeatureCNN(num_classes).to(DEVICE)\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIZDFbEyidqG",
        "outputId": "80e9fd85-8fab-4a9c-a98f-a05b57e2d2aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 01] Train loss: 4.3063 | Train acc: 0.1433 || Val loss: 2.0835 | Val acc: 0.1342\n",
            "New best model saved (val_acc=0.1342)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 02] Train loss: 2.0824 | Train acc: 0.1336 || Val loss: 2.0813 | Val acc: 0.1212\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 03] Train loss: 2.0852 | Train acc: 0.1314 || Val loss: 2.0797 | Val acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 04] Train loss: 2.0795 | Train acc: 0.1336 || Val loss: 2.0781 | Val acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 05] Train loss: 2.0779 | Train acc: 0.1336 || Val loss: 2.0765 | Val acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 06] Train loss: 2.0767 | Train acc: 0.1336 || Val loss: 2.0752 | Val acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 07] Train loss: 2.0757 | Train acc: 0.1336 || Val loss: 2.0737 | Val acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 08] Train loss: 2.0818 | Train acc: 0.1346 || Val loss: 2.0725 | Val acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 09] Train loss: 2.0734 | Train acc: 0.1336 || Val loss: 2.0714 | Val acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 10] Train loss: 2.0724 | Train acc: 0.1336 || Val loss: 2.0703 | Val acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 11] Train loss: 2.0715 | Train acc: 0.1336 || Val loss: 2.0693 | Val acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 12] Train loss: 2.0707 | Train acc: 0.1336 || Val loss: 2.0685 | Val acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 13] Train loss: 2.0702 | Train acc: 0.1336 || Val loss: 2.0675 | Val acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 14] Train loss: 2.0693 | Train acc: 0.1336 || Val loss: 2.0668 | Val acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 15] Train loss: 2.0687 | Train acc: 0.1336 || Val loss: 2.0661 | Val acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 16] Train loss: 2.0681 | Train acc: 0.1336 || Val loss: 2.0653 | Val acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 17] Train loss: 2.0675 | Train acc: 0.1336 || Val loss: 2.0647 | Val acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 18] Train loss: 2.0671 | Train acc: 0.1336 || Val loss: 2.0641 | Val acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 19] Train loss: 2.0666 | Train acc: 0.1336 || Val loss: 2.0634 | Val acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 20] Train loss: 2.0663 | Train acc: 0.1336 || Val loss: 2.0630 | Val acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 21] Train loss: 2.0659 | Train acc: 0.1336 || Val loss: 2.0625 | Val acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 22] Train loss: 2.0656 | Train acc: 0.1336 || Val loss: 2.0620 | Val acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 23] Train loss: 2.0653 | Train acc: 0.1336 || Val loss: 2.0616 | Val acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 24] Train loss: 2.0650 | Train acc: 0.1336 || Val loss: 2.0612 | Val acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 25] Train loss: 2.0645 | Train acc: 0.1336 || Val loss: 2.0609 | Val acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 26] Train loss: 2.0645 | Train acc: 0.1336 || Val loss: 2.0606 | Val acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 27] Train loss: 2.0642 | Train acc: 0.1336 || Val loss: 2.0601 | Val acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 28] Train loss: 2.0642 | Train acc: 0.1336 || Val loss: 2.0598 | Val acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 29] Train loss: 2.0638 | Train acc: 0.1336 || Val loss: 2.0596 | Val acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 30] Train loss: 2.0634 | Train acc: 0.1336 || Val loss: 2.0595 | Val acc: 0.1342\n",
            "Training complete. Best validation accuracy: 0.1341991341991342\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler() if DEVICE == \"cuda\" else None\n",
        "\n",
        "best_val_acc = 0.0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # ===== TRAINING =====\n",
        "    model.train()\n",
        "    total_loss, correct = 0.0, 0\n",
        "\n",
        "    for mel, mfcc, chroma, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=False):\n",
        "        mel, mfcc, chroma, labels = mel.to(DEVICE), mfcc.to(DEVICE), chroma.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        with torch.amp.autocast(\"cuda\", enabled=(DEVICE == \"cuda\")):\n",
        "            outputs = model(mel, mfcc, chroma)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        if scaler: \n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        correct += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    train_acc = correct / len(train_dataset)\n",
        "\n",
        "    # validate\n",
        "    model.eval()\n",
        "    val_correct, val_loss = 0, 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for mel, mfcc, chroma, labels in val_loader:\n",
        "            mel, mfcc, chroma, labels = mel.to(DEVICE), mfcc.to(DEVICE), chroma.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = model(mel, mfcc, chroma)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            val_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_acc = val_correct / len(val_dataset)\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "    print(f\"[Epoch {epoch+1:02d}] \"\n",
        "          f\"Train loss: {avg_train_loss:.4f} | Train acc: {train_acc:.4f} || \"\n",
        "          f\"Val loss: {avg_val_loss:.4f} | Val acc: {val_acc:.4f}\")\n",
        "\n",
        "    #Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")\n",
        "        print(f\"New best model saved (val_acc={val_acc:.4f})\")\n",
        "\n",
        "print(\"Training complete. Best validation accuracy:\", best_val_acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DV_Lfqymir4I"
      },
      "outputs": [],
      "source": [
        "# torch.save(model.state_dict(), \"/content/drive/MyDrive/Speech_Emotion_Recognition/model_cnn_multi.pth\")\n",
        "# print(\"Model saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pg1U54VevSzm"
      },
      "outputs": [],
      "source": [
        "#test model ---> run this cell\n",
        "import os\n",
        "import re\n",
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ===== Cấu hình =====\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/Speech_Emotion_Recognition/model_cnn_multi.pth\" #<---- thay doi duong dan file mode.pth\n",
        "TEST_DIR = \"/content/drive/MyDrive/Speech_Emotion_Recognition/working/processed/test\" #<---- thay doi duong dan\n",
        "OUTPUT_CSV = \"/content/drive/MyDrive/Speech_Emotion_Recognition/predictions.csv\" #<---- thay doi duong dan\n",
        "NUM_CLASSES = 8  #8 emotion\n",
        "\n",
        "#Mapping emotion_map\n",
        "EMOTION_MAP = {\n",
        "    '01': 'neutral',\n",
        "    '02': 'calm',\n",
        "    '03': 'happy',\n",
        "    '04': 'sad',\n",
        "    '05': 'angry',\n",
        "    '06': 'fearful',\n",
        "    '07': 'disgust',\n",
        "    '08': 'surprised'\n",
        "}\n",
        "\n",
        "def pad_or_trim(feature, target_frames=128):\n",
        "    if feature.shape[1] < target_frames:\n",
        "        pad_width = target_frames - feature.shape[1]\n",
        "        return np.pad(feature, ((0, 0), (0, pad_width)), mode='constant')\n",
        "    else:\n",
        "        return feature[:, :target_frames]\n",
        "\n",
        "def load_model():\n",
        "    model = MultiFeatureCNN(NUM_CLASSES).to(DEVICE)\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def extract_emotion_code(filename):\n",
        "    match = re.match(r'^\\d{2}-\\d{2}-(\\d{2})-', filename)\n",
        "    if match:\n",
        "        code = match.group(1)\n",
        "        return EMOTION_MAP.get(code, \"unknown\")\n",
        "    return \"unknown\"\n",
        "\n",
        "def predict_emotion(audio_path, model):\n",
        "    y, sr = librosa.load(audio_path, sr=16000, mono=True)\n",
        "\n",
        "    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
        "    mel = librosa.power_to_db(mel, ref=np.max)\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "\n",
        "    mel = pad_or_trim(mel, 128)\n",
        "    mfcc = pad_or_trim(mfcc, 128)\n",
        "    chroma = pad_or_trim(chroma, 128)\n",
        "\n",
        "    mel = torch.tensor(mel).unsqueeze(0).unsqueeze(0).float().to(DEVICE)\n",
        "    mfcc = torch.tensor(mfcc).unsqueeze(0).unsqueeze(0).float().to(DEVICE)\n",
        "    chroma = torch.tensor(chroma).unsqueeze(0).unsqueeze(0).float().to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(mel, mfcc, chroma)\n",
        "        probs = F.softmax(output, dim=1)\n",
        "        pred = torch.argmax(probs, dim=1).item()\n",
        "        confidence = probs[0, pred].item()\n",
        "\n",
        "    return pred, confidence\n",
        "\n",
        "def predict_all_test_files():\n",
        "    model = load_model()\n",
        "    results = []\n",
        "    wav_files = [f for f in os.listdir(TEST_DIR) if f.endswith(\".wav\")]\n",
        "\n",
        "    for fname in tqdm(wav_files, desc=\"Predicting\"):\n",
        "        file_path = os.path.join(TEST_DIR, fname)\n",
        "        try:\n",
        "            pred_idx, confidence = predict_emotion(file_path, model)\n",
        "            predicted_label = list(EMOTION_MAP.values())[pred_idx] if pred_idx < len(EMOTION_MAP) else f\"unknown({pred_idx})\"\n",
        "            true_label = extract_emotion_code(fname)\n",
        "            results.append({\n",
        "                \"file_name\": fname,\n",
        "                \"file_path\": file_path,\n",
        "                \"true_label\": true_label,\n",
        "                \"predicted_label\": predicted_label,\n",
        "                \"confidence\": round(confidence, 4)\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Error {file_path}: {e}\")\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(OUTPUT_CSV, index=False)\n",
        "    print(f\"\\nSaved to {OUTPUT_CSV}\")\n",
        "    return df\n",
        "\n",
        "pred_df = predict_all_test_files()\n",
        "pred_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdDHitxZ_8Ms"
      },
      "source": [
        "#Sử dụng model pre-train\n",
        "acc = 0.4545"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWq80ExPACYv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model, Wav2Vec2FeatureExtractor\n",
        "import pandas as pd\n",
        "import librosa\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgjL4R-SCkzJ",
        "outputId": "e40c5294-f254-4873-f69e-f69f06e2ed0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thiết bị: cuda\n"
          ]
        }
      ],
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DATA_DIR = \"/content/drive/MyDrive/Speech_Emotion_Recognition/working/processed\" #<---- thay doi duong dan\n",
        "MODEL_SAVE_PATH = \"/content/drive/MyDrive/Speech_Emotion_Recognition/wav2vec_emotion.pth\" #<---- thay doi duong dan\n",
        "SAMPLE_RATE = 16000\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 20\n",
        "LR = 1e-4\n",
        "\n",
        "print(\"Device: \", DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5UOCo_pD2d9",
        "outputId": "4153ce26-57fb-480e-f83d-e113295c18b8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "base_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjIUqQi0D3AF",
        "outputId": "bc32fca0-dbf6-43c6-d634-de357fdcc7ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label mapping: {'angry': 0, 'calm': 1, 'disgust': 2, 'fearful': 3, 'happy': 4, 'neutral': 5, 'sad': 6, 'surprised': 7}\n"
          ]
        }
      ],
      "source": [
        "train_csv = os.path.join(DATA_DIR, \"train_final.csv\")\n",
        "val_csv = os.path.join(DATA_DIR, \"val_final.csv\")\n",
        "\n",
        "train_df = pd.read_csv(train_csv)\n",
        "val_df = pd.read_csv(val_csv)\n",
        "\n",
        "# Lấy danh sách nhãn\n",
        "unique_labels = sorted(train_df[\"label\"].unique())\n",
        "label2id = {lbl: i for i, lbl in enumerate(unique_labels)}\n",
        "id2label = {i: lbl for lbl, i in label2id.items()}\n",
        "NUM_CLASSES = len(label2id)\n",
        "\n",
        "print(\"Label mapping:\", label2id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RBfUpUK2_mXp",
        "outputId": "a3cd3f02-3eb0-425b-8212-d91e8d61e7ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: audiomentations in /usr/local/lib/python3.12/dist-packages (0.43.1)\n",
            "Requirement already satisfied: numpy<3,>=1.22.0 in /usr/local/lib/python3.12/dist-packages (from audiomentations) (2.0.2)\n",
            "Requirement already satisfied: numpy-minmax<1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from audiomentations) (0.5.0)\n",
            "Requirement already satisfied: numpy-rms<1,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from audiomentations) (0.6.0)\n",
            "Requirement already satisfied: librosa!=0.10.0,<0.12.0,>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from audiomentations) (0.11.0)\n",
            "Requirement already satisfied: python-stretch<1,>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from audiomentations) (0.3.1)\n",
            "Requirement already satisfied: scipy<2,>=1.4 in /usr/local/lib/python3.12/dist-packages (from audiomentations) (1.16.2)\n",
            "Requirement already satisfied: soxr<1.0.0,>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from audiomentations) (0.5.0.post1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (0.60.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (1.8.2)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (4.15.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (1.1.2)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from numpy-minmax<1,>=0.3.0->audiomentations) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0.0->numpy-minmax<1,>=0.3.0->audiomentations) (2.23)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from lazy_loader>=0.1->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (25.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (4.5.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (2.32.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (3.6.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (2025.10.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install audiomentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGU9LE9fQT-E"
      },
      "outputs": [],
      "source": [
        "from audiomentations import Compose, AddGaussianNoise, PitchShift, TimeStretch\n",
        "\n",
        "augment = Compose([\n",
        "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.3),\n",
        "    PitchShift(min_semitones=-2, max_semitones=2, p=0.4),\n",
        "    TimeStretch(min_rate=0.8, max_rate=1.2, p=0.3)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlV7gyDKEFmH"
      },
      "outputs": [],
      "source": [
        "class SpeechEmotionDataset(Dataset):\n",
        "    def __init__(self, df, extractor):\n",
        "        self.data = df\n",
        "        self.extractor = extractor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        path = row[\"file_path\"]\n",
        "        label = int(row[\"label_id\"])\n",
        "\n",
        "        # Load audio\n",
        "        y, sr = librosa.load(path, sr=SAMPLE_RATE, mono=True)\n",
        "\n",
        "        # Extract Wav2Vec2 features\n",
        "        inputs = self.extractor(y, sampling_rate=SAMPLE_RATE, return_tensors=\"pt\", padding=True)\n",
        "        input_values = inputs[\"input_values\"][0]\n",
        "        return input_values, torch.tensor(label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beiTr0dnEryz"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    input_values = [b[0] for b in batch]\n",
        "    labels = torch.stack([b[1] for b in batch])\n",
        "\n",
        "    # Dùng extractor.pad để tạo batch có cùng độ dài\n",
        "    padded = extractor.pad(\n",
        "        {\"input_values\": input_values},\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    return padded[\"input_values\"], labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfUKHfCiEI3b"
      },
      "outputs": [],
      "source": [
        "class Wav2Vec2EmotionClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.wav2vec = base_model\n",
        "        for param in self.wav2vec.parameters():\n",
        "            param.requires_grad = False  # freeze để train nhanh hơn\n",
        "\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(768, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_values):\n",
        "        # Giữ nguyên Wav2Vec2, chỉ train classifier\n",
        "        with torch.no_grad():\n",
        "            features = self.wav2vec(input_values).last_hidden_state\n",
        "        x = features.mean(dim=1)\n",
        "        return self.classifier(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-RM3rYsELTR"
      },
      "outputs": [],
      "source": [
        "train_dataset = SpeechEmotionDataset(train_df, extractor)\n",
        "val_dataset = SpeechEmotionDataset(val_df, extractor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5-S7FrJEPXN"
      },
      "outputs": [],
      "source": [
        "model = Wav2Vec2EmotionClassifier(NUM_CLASSES).to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBXzFfIYER7N",
        "outputId": "17a4a895-c20b-47d0-de6f-4060988d19d4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 1/20:   0%|          | 0/116 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "Epoch 1/20: 100%|██████████| 116/116 [09:45<00:00,  5.05s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Train loss: 2.0318 | acc: 0.2269\n",
            "Val loss: 1.9771, acc: 0.3117\n",
            "Saved new best model (0.3117)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20: 100%|██████████| 116/116 [00:21<00:00,  5.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 2] Train loss: 1.9503 | acc: 0.2986\n",
            "Val loss: 1.8977, acc: 0.3290\n",
            "Saved new best model (0.3290)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/20: 100%|██████████| 116/116 [00:25<00:00,  4.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 3] Train loss: 1.8818 | acc: 0.3116\n",
            "Val loss: 1.8364, acc: 0.3074\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/20: 100%|██████████| 116/116 [00:21<00:00,  5.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 4] Train loss: 1.8221 | acc: 0.3301\n",
            "Val loss: 1.7937, acc: 0.3074\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/20: 100%|██████████| 116/116 [00:21<00:00,  5.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 5] Train loss: 1.7826 | acc: 0.3268\n",
            "Val loss: 1.7647, acc: 0.3160\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/20: 100%|██████████| 116/116 [00:21<00:00,  5.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 6] Train loss: 1.7525 | acc: 0.3290\n",
            "Val loss: 1.7375, acc: 0.3593\n",
            "Saved new best model (0.3593)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/20: 100%|██████████| 116/116 [00:25<00:00,  4.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 7] Train loss: 1.7279 | acc: 0.3344\n",
            "Val loss: 1.7192, acc: 0.3723\n",
            "Saved new best model (0.3723)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/20: 100%|██████████| 116/116 [00:24<00:00,  4.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 8] Train loss: 1.7124 | acc: 0.3594\n",
            "Val loss: 1.7077, acc: 0.3723\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/20: 100%|██████████| 116/116 [00:21<00:00,  5.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 9] Train loss: 1.6823 | acc: 0.3605\n",
            "Val loss: 1.6950, acc: 0.3766\n",
            "Saved new best model (0.3766)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/20: 100%|██████████| 116/116 [00:24<00:00,  4.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 10] Train loss: 1.6756 | acc: 0.3702\n",
            "Val loss: 1.6747, acc: 0.3983\n",
            "Saved new best model (0.3983)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/20: 100%|██████████| 116/116 [00:24<00:00,  4.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 11] Train loss: 1.6502 | acc: 0.3648\n",
            "Val loss: 1.6623, acc: 0.3896\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/20: 100%|██████████| 116/116 [00:21<00:00,  5.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 12] Train loss: 1.6316 | acc: 0.3724\n",
            "Val loss: 1.6449, acc: 0.4286\n",
            "Saved new best model (0.4286)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/20: 100%|██████████| 116/116 [00:24<00:00,  4.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 13] Train loss: 1.6237 | acc: 0.4007\n",
            "Val loss: 1.6360, acc: 0.3939\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/20: 100%|██████████| 116/116 [00:21<00:00,  5.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 14] Train loss: 1.6083 | acc: 0.3898\n",
            "Val loss: 1.6172, acc: 0.3939\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/20: 100%|██████████| 116/116 [00:21<00:00,  5.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 15] Train loss: 1.6024 | acc: 0.3952\n",
            "Val loss: 1.6072, acc: 0.4069\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16/20: 100%|██████████| 116/116 [00:23<00:00,  4.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 16] Train loss: 1.5726 | acc: 0.4300\n",
            "Val loss: 1.6018, acc: 0.3896\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17/20: 100%|██████████| 116/116 [00:21<00:00,  5.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 17] Train loss: 1.5639 | acc: 0.4017\n",
            "Val loss: 1.5833, acc: 0.4286\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18/20: 100%|██████████| 116/116 [00:21<00:00,  5.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 18] Train loss: 1.5441 | acc: 0.4093\n",
            "Val loss: 1.5826, acc: 0.4156\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19/20: 100%|██████████| 116/116 [00:21<00:00,  5.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 19] Train loss: 1.5465 | acc: 0.4169\n",
            "Val loss: 1.5774, acc: 0.4199\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20/20: 100%|██████████| 116/116 [00:21<00:00,  5.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 20] Train loss: 1.5394 | acc: 0.4267\n",
            "Val loss: 1.5687, acc: 0.4199\n",
            "Training complete. Best val acc = 0.4286\n"
          ]
        }
      ],
      "source": [
        "best_val_acc = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    train_loss, correct = 0, 0\n",
        "\n",
        "    for input_values, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
        "        input_values, labels = input_values.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_values)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        correct += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "    train_acc = correct / len(train_dataset)\n",
        "    print(f\"[Epoch {epoch+1}] Train loss: {train_loss/len(train_loader):.4f} | acc: {train_acc:.4f}\")\n",
        "\n",
        "    # ===== Validation =====\n",
        "    model.eval()\n",
        "    val_correct, val_loss = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for input_values, labels in val_loader:\n",
        "            input_values, labels = input_values.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = model(input_values)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            val_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "    val_acc = val_correct / len(val_dataset)\n",
        "    print(f\"Val loss: {val_loss/len(val_loader):.4f}, acc: {val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "        print(f\"Saved new best model ({best_val_acc:.4f})\")\n",
        "\n",
        "print(f\"Training complete. Best val acc = {best_val_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "80wza9wMRbZo",
        "outputId": "62102364-ea2a-4760-e5bf-cb8add81755c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test samples: 288\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Predicting: 100%|██████████| 36/36 [08:23<00:00, 13.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.1319\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral       0.00      0.00      0.00        19\n",
            "        calm       0.00      0.00      0.00        38\n",
            "       happy       0.13      1.00      0.23        38\n",
            "         sad       0.00      0.00      0.00        38\n",
            "       angry       0.00      0.00      0.00        39\n",
            "     fearful       0.00      0.00      0.00        39\n",
            "     disgust       0.00      0.00      0.00        38\n",
            "   surprised       0.00      0.00      0.00        39\n",
            "\n",
            "    accuracy                           0.13       288\n",
            "   macro avg       0.02      0.12      0.03       288\n",
            "weighted avg       0.02      0.13      0.03       288\n",
            "\n",
            "Saved predictions to: /content/drive/MyDrive/Speech_Emotion_Recognition/test_predictions.csv\n",
            "                                           file_path  true_label_id  \\\n",
            "0  /content/drive/MyDrive/Speech_Emotion_Recognit...              2   \n",
            "1  /content/drive/MyDrive/Speech_Emotion_Recognit...              7   \n",
            "2  /content/drive/MyDrive/Speech_Emotion_Recognit...              7   \n",
            "3  /content/drive/MyDrive/Speech_Emotion_Recognit...              6   \n",
            "4  /content/drive/MyDrive/Speech_Emotion_Recognit...              2   \n",
            "\n",
            "  true_label  pred_label_id pred_label  \n",
            "0      happy              2      happy  \n",
            "1  surprised              2      happy  \n",
            "2  surprised              2      happy  \n",
            "3    disgust              2      happy  \n",
            "4      happy              2      happy  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# import os\n",
        "# import torch\n",
        "# import pandas as pd\n",
        "# import librosa\n",
        "# import numpy as np\n",
        "# from torch import nn\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from torch.nn.utils.rnn import pad_sequence\n",
        "# from transformers import Wav2Vec2Model, Wav2Vec2FeatureExtractor\n",
        "# from tqdm import tqdm\n",
        "# from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# # =============================\n",
        "# # 1Cấu hình\n",
        "# # =============================\n",
        "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# SAMPLE_RATE = 16000\n",
        "# BATCH_SIZE = 8\n",
        "# NUM_CLASSES = 8\n",
        "# MODEL_PATH = \"/content/drive/MyDrive/Speech_Emotion_Recognition/wav2vec2_aug_best.pth\"\n",
        "# TEST_CSV = \"/content/drive/MyDrive/Speech_Emotion_Recognition/working/processed/test_final.csv\"\n",
        "# OUTPUT_CSV = \"/content/drive/MyDrive/Speech_Emotion_Recognition/test_predictions.csv\"\n",
        "# BASE_DIR = \"/content/drive/MyDrive\"  # Đường dẫn gốc để nối vào CSV\n",
        "\n",
        "# # =============================\n",
        "# # 2. mapping cảm xúc\n",
        "# # =============================\n",
        "# ID2LABEL = {\n",
        "#     0: \"neutral\",\n",
        "#     1: \"calm\",\n",
        "#     2: \"happy\",\n",
        "#     3: \"sad\",\n",
        "#     4: \"angry\",\n",
        "#     5: \"fearful\",\n",
        "#     6: \"disgust\",\n",
        "#     7: \"surprised\"\n",
        "# }\n",
        "\n",
        "# # =============================\n",
        "# # 3. Feature extractor\n",
        "# # =============================\n",
        "# extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "\n",
        "# # =============================\n",
        "# # 4. Dataset test\n",
        "# # =============================\n",
        "# class TestDataset(Dataset):\n",
        "#     def __init__(self, csv_path, extractor, base_dir):\n",
        "#         self.data = pd.read_csv(csv_path)\n",
        "#         self.extractor = extractor\n",
        "#         self.base_dir = base_dir\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         row = self.data.iloc[idx]\n",
        "#         # Ghép đường dẫn tuyệt đối\n",
        "#         rel_path = row[\"file_path\"].lstrip(\"/\")\n",
        "#         full_path = os.path.join(self.base_dir, rel_path)\n",
        "#         label = int(row[\"label_id\"])\n",
        "\n",
        "#         # Đọc file audio\n",
        "#         y, sr = librosa.load(full_path, sr=SAMPLE_RATE, mono=True)\n",
        "#         inputs = self.extractor(y, sampling_rate=SAMPLE_RATE, return_tensors=\"pt\", padding=True)\n",
        "#         input_values = inputs.input_values.squeeze(0)\n",
        "#         return {\"input_values\": input_values, \"label\": label, \"file_path\": full_path}\n",
        "\n",
        "# def collate_fn(batch):\n",
        "#     input_values = [item[\"input_values\"] for item in batch]\n",
        "#     labels = torch.tensor([item[\"label\"] for item in batch], dtype=torch.long)\n",
        "#     file_paths = [item[\"file_path\"] for item in batch]\n",
        "#     input_values = pad_sequence(input_values, batch_first=True)\n",
        "#     return {\"input_values\": input_values, \"labels\": labels, \"file_paths\": file_paths}\n",
        "\n",
        "# test_dataset = TestDataset(TEST_CSV, extractor, BASE_DIR)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "# print(f\"Test samples: {len(test_dataset)}\")\n",
        "\n",
        "# # =============================\n",
        "# # 5️. Mô hình\n",
        "# # =============================\n",
        "# class Wav2VecEmotion(nn.Module):\n",
        "#     def __init__(self, num_classes=8):\n",
        "#         super().__init__()\n",
        "#         self.wav2vec = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "#         self.classifier = nn.Sequential(\n",
        "#             nn.Linear(self.wav2vec.config.hidden_size, 256),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(0.3),\n",
        "#             nn.Linear(256, num_classes)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, input_values):\n",
        "#         outputs = self.wav2vec(input_values)\n",
        "#         hidden_states = outputs.last_hidden_state\n",
        "#         x = hidden_states.mean(dim=1)\n",
        "#         logits = self.classifier(x)\n",
        "#         return logits\n",
        "\n",
        "# # =============================\n",
        "# # 6️. Load model\n",
        "# # =============================\n",
        "# model = Wav2VecEmotion(NUM_CLASSES).to(DEVICE)\n",
        "# model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "# model.eval()\n",
        "# print(\"Model loaded successfully.\")\n",
        "\n",
        "# # =============================\n",
        "# # 7️. Dự đoán\n",
        "# # =============================\n",
        "# preds, truths, files = [], [], []\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
        "#         input_values = batch[\"input_values\"].to(DEVICE)\n",
        "#         labels = batch[\"labels\"].cpu().numpy()\n",
        "\n",
        "#         outputs = model(input_values)\n",
        "#         pred_labels = outputs.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "#         preds.extend(pred_labels)\n",
        "#         truths.extend(labels)\n",
        "#         files.extend(batch[\"file_paths\"])\n",
        "\n",
        "# # =============================\n",
        "# # 8️. Tính accuracy & xuất kết quả\n",
        "# # =============================\n",
        "# acc = accuracy_score(truths, preds)\n",
        "# print(f\"Test Accuracy: {acc:.4f}\")\n",
        "# print(\"Classification Report:\")\n",
        "# print(classification_report(truths, preds, target_names=list(ID2LABEL.values())))\n",
        "\n",
        "# results = pd.DataFrame({\n",
        "#     \"file_path\": files,\n",
        "#     \"true_label_id\": truths,\n",
        "#     \"true_label\": [ID2LABEL[i] for i in truths],\n",
        "#     \"pred_label_id\": preds,\n",
        "#     \"pred_label\": [ID2LABEL[i] for i in preds]\n",
        "# })\n",
        "\n",
        "# results.to_csv(OUTPUT_CSV, index=False)\n",
        "# print(f\"Saved predictions to: {OUTPUT_CSV}\")\n",
        "# print(results.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFBg6jXYAp5C"
      },
      "outputs": [],
      "source": [
        "preds, files = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
        "        input_values = batch[\"input_values\"].to(DEVICE)\n",
        "        outputs = model(input_values)\n",
        "        pred_labels = outputs.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "        preds.extend(pred_labels)\n",
        "        files.extend(batch[\"file_paths\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baShfkCRRRD-"
      },
      "outputs": [],
      "source": [
        "results = pd.DataFrame({\n",
        "    \"file_path\": files,\n",
        "    \"pred_label_id\": preds,\n",
        "    \"pred_label_name\": [ID2LABEL[i] for i in preds]\n",
        "})\n",
        "\n",
        "results.to_csv(OUTPUT_CSV, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ufObKr0SxiS"
      },
      "source": [
        "#Sử dụng model pretrain với data augment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ev-0ZcljTHlm",
        "outputId": "4b4954b2-c612-46c8-aeb5-9b1ba011c9ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: audiomentations in /usr/local/lib/python3.12/dist-packages (0.43.1)\n",
            "Requirement already satisfied: numpy<3,>=1.22.0 in /usr/local/lib/python3.12/dist-packages (from audiomentations) (2.0.2)\n",
            "Requirement already satisfied: numpy-minmax<1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from audiomentations) (0.5.0)\n",
            "Requirement already satisfied: numpy-rms<1,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from audiomentations) (0.6.0)\n",
            "Requirement already satisfied: librosa!=0.10.0,<0.12.0,>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from audiomentations) (0.11.0)\n",
            "Requirement already satisfied: python-stretch<1,>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from audiomentations) (0.3.1)\n",
            "Requirement already satisfied: scipy<2,>=1.4 in /usr/local/lib/python3.12/dist-packages (from audiomentations) (1.16.2)\n",
            "Requirement already satisfied: soxr<1.0.0,>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from audiomentations) (0.5.0.post1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (0.60.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (1.8.2)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (4.15.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (1.1.2)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from numpy-minmax<1,>=0.3.0->audiomentations) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0.0->numpy-minmax<1,>=0.3.0->audiomentations) (2.23)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from lazy_loader>=0.1->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (25.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (4.5.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (2.32.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (3.6.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (2025.10.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install audiomentations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBp8MieaS3po"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import librosa\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import Wav2Vec2Model, Wav2Vec2FeatureExtractor\n",
        "from audiomentations import Compose, AddGaussianNoise, PitchShift, TimeStretch\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6bnP5NOTAUB",
        "outputId": "2e570554-0a8c-45da-e87b-2bd9400f047b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "SAMPLE_RATE = 16000\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 20\n",
        "LR = 1e-4\n",
        "NUM_CLASSES = 8\n",
        "MODEL_SAVE_PATH = \"/content/drive/MyDrive/Speech_Emotion_Recognition/wav2vec2_aug_best.pth\"\n",
        "\n",
        "print(\"Device:\", DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXbEArnVaSBg"
      },
      "outputs": [],
      "source": [
        "EMOTION_MAP = {\n",
        "    \"neutral\": 0,\n",
        "    \"calm\": 1,\n",
        "    \"happy\": 2,\n",
        "    \"sad\": 3,\n",
        "    \"angry\": 4,\n",
        "    \"fearful\": 5,\n",
        "    \"disgust\": 6,\n",
        "    \"surprised\": 7\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNHI3G-rbLw5"
      },
      "outputs": [],
      "source": [
        "augment = Compose([\n",
        "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.3),\n",
        "    PitchShift(min_semitones=-2, max_semitones=2, p=0.3),\n",
        "    TimeStretch(min_rate=0.8, max_rate=1.25, p=0.3)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ru3vqGWTVYM"
      },
      "outputs": [],
      "source": [
        "class SpeechEmotionDataset(Dataset):\n",
        "    def __init__(self, csv_path, extractor, augment=None):\n",
        "        self.data = pd.read_csv(csv_path)\n",
        "        self.extractor = extractor\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        path = row[\"file_path\"]\n",
        "\n",
        "        # Load audio\n",
        "        y, sr = librosa.load(path, sr=SAMPLE_RATE, mono=True)\n",
        "\n",
        "        # Apply augment\n",
        "        if self.augment:\n",
        "            y = self.augment(samples=y, sample_rate=SAMPLE_RATE)\n",
        "\n",
        "        # Extract features\n",
        "        inputs = self.extractor(y, sampling_rate=SAMPLE_RATE, return_tensors=\"pt\", padding=True)\n",
        "        input_values = inputs.input_values.squeeze(0)  # (seq_len,)\n",
        "\n",
        "        label = torch.tensor(int(row[\"label_id\"]), dtype=torch.long)\n",
        "\n",
        "        return {\"input_values\": input_values, \"labels\": label}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HMR6vrRbQ32"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    input_values = [item[\"input_values\"] for item in batch]\n",
        "    labels = torch.stack([item[\"labels\"] for item in batch])\n",
        "\n",
        "    # Pad audio tensor về cùng độ dài trong batch\n",
        "    input_values = pad_sequence(input_values, batch_first=True)\n",
        "\n",
        "    return {\"input_values\": input_values, \"labels\": labels}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUsEzQiEbUg7",
        "outputId": "a70e8996-b63b-47d0-bad9-8bc3c35322ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train samples: 921, Val samples: 231\n"
          ]
        }
      ],
      "source": [
        "train_csv = \"/content/drive/MyDrive/Speech_Emotion_Recognition/working/processed/train_final.csv\"\n",
        "val_csv   = \"/content/drive/MyDrive/Speech_Emotion_Recognition/working/processed/val_final.csv\"\n",
        "\n",
        "extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "\n",
        "train_dataset = SpeechEmotionDataset(train_csv, extractor, augment=augment)\n",
        "val_dataset   = SpeechEmotionDataset(val_csv, extractor, augment=None)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkDYeH8gTa4Y"
      },
      "outputs": [],
      "source": [
        "class Wav2VecEmotion(nn.Module):\n",
        "    def __init__(self, num_classes=8):\n",
        "        super().__init__()\n",
        "        self.wav2vec = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.wav2vec.config.hidden_size, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_values):\n",
        "        with torch.set_grad_enabled(True):\n",
        "            outputs = self.wav2vec(input_values)\n",
        "            hidden_states = outputs.last_hidden_state\n",
        "            x = hidden_states.mean(dim=1)  # Mean pooling\n",
        "            logits = self.classifier(x)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5ua7cp1Z3yU",
        "outputId": "7a763765-a77e-4b0e-a7df-cc0d10e14785"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/20: 100%|██████████| 116/116 [08:56<00:00,  4.63s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Train loss: 2.0777 | acc: 0.1336 || Val loss: 2.0606 | acc: 0.1342\n",
            "Saved new best model (0.1342)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20: 100%|██████████| 116/116 [01:31<00:00,  1.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 2] Train loss: 2.0732 | acc: 0.1314 || Val loss: 2.0610 | acc: 0.1299\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/20: 100%|██████████| 116/116 [01:31<00:00,  1.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 3] Train loss: 2.0703 | acc: 0.1194 || Val loss: 2.0610 | acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/20: 100%|██████████| 116/116 [01:32<00:00,  1.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 4] Train loss: 2.0696 | acc: 0.1281 || Val loss: 2.0612 | acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/20: 100%|██████████| 116/116 [01:32<00:00,  1.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 5] Train loss: 2.0641 | acc: 0.1390 || Val loss: 2.0601 | acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/20: 100%|██████████| 116/116 [01:32<00:00,  1.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 6] Train loss: 2.0651 | acc: 0.1281 || Val loss: 2.0604 | acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/20: 100%|██████████| 116/116 [01:31<00:00,  1.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 7] Train loss: 2.0624 | acc: 0.1238 || Val loss: 2.0604 | acc: 0.1299\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/20: 100%|██████████| 116/116 [01:31<00:00,  1.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 8] Train loss: 2.0655 | acc: 0.1249 || Val loss: 2.0602 | acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/20: 100%|██████████| 116/116 [01:31<00:00,  1.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 9] Train loss: 2.0656 | acc: 0.1238 || Val loss: 2.0603 | acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/20: 100%|██████████| 116/116 [01:32<00:00,  1.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 10] Train loss: 2.0652 | acc: 0.1292 || Val loss: 2.0600 | acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/20: 100%|██████████| 116/116 [01:31<00:00,  1.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 11] Train loss: 2.0638 | acc: 0.1249 || Val loss: 2.0600 | acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/20: 100%|██████████| 116/116 [01:32<00:00,  1.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 12] Train loss: 2.0752 | acc: 0.1238 || Val loss: 2.0606 | acc: 0.1299\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/20: 100%|██████████| 116/116 [01:32<00:00,  1.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 13] Train loss: 2.0667 | acc: 0.1368 || Val loss: 2.0609 | acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/20: 100%|██████████| 116/116 [01:31<00:00,  1.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 14] Train loss: 2.0640 | acc: 0.1260 || Val loss: 2.0605 | acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/20: 100%|██████████| 116/116 [01:32<00:00,  1.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 15] Train loss: 2.0656 | acc: 0.1194 || Val loss: 2.0604 | acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16/20: 100%|██████████| 116/116 [01:31<00:00,  1.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 16] Train loss: 2.0664 | acc: 0.1183 || Val loss: 2.0599 | acc: 0.1299\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17/20: 100%|██████████| 116/116 [01:32<00:00,  1.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 17] Train loss: 2.0624 | acc: 0.1401 || Val loss: 2.0598 | acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18/20: 100%|██████████| 116/116 [01:31<00:00,  1.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 18] Train loss: 2.0641 | acc: 0.1292 || Val loss: 2.0604 | acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19/20: 100%|██████████| 116/116 [01:31<00:00,  1.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 19] Train loss: 2.0680 | acc: 0.1227 || Val loss: 2.0609 | acc: 0.1342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20/20: 100%|██████████| 116/116 [01:31<00:00,  1.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 20] Train loss: 2.0677 | acc: 0.1129 || Val loss: 2.0606 | acc: 0.1342\n",
            "Training complete. Best val acc = 0.1342\n"
          ]
        }
      ],
      "source": [
        "model = Wav2VecEmotion(NUM_CLASSES).to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "best_val_acc = 0\n",
        "\n",
        "print(\"-------------------------------------------------\")\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    train_loss, correct = 0, 0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
        "        input_values = batch[\"input_values\"].to(DEVICE)\n",
        "        labels = batch[\"labels\"].to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_values)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        correct += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "    train_acc = correct / len(train_dataset)\n",
        "\n",
        "    # ===== Validation =====\n",
        "    model.eval()\n",
        "    val_loss, val_correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_values = batch[\"input_values\"].to(DEVICE)\n",
        "            labels = batch[\"labels\"].to(DEVICE)\n",
        "            outputs = model(input_values)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            val_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "    val_acc = val_correct / len(val_dataset)\n",
        "\n",
        "    print(f\"[Epoch {epoch+1}] Train loss: {train_loss/len(train_loader):.4f} | acc: {train_acc:.4f} || \"\n",
        "          f\"Val loss: {val_loss/len(val_loader):.4f} | acc: {val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "        print(f\"Saved new best model ({best_val_acc:.4f})\")\n",
        "\n",
        "print(f\"Training complete. Best val acc = {best_val_acc:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
